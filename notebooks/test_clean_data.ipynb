{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e1ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "from hydro_forecasting.data.preprocessing import ProcessingConfig\n",
    "from hydro_forecasting.data.clean_data import clean_data, save_quality_report_to_json\n",
    "from hydro_forecasting.preprocessing.time_series_preprocessing import (\n",
    "    fit_time_series_pipelines,\n",
    "    transform_time_series_data,\n",
    "    save_time_series_pipelines,\n",
    "    load_time_series_pipelines,\n",
    ")\n",
    "\n",
    "from returns.result import Success\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3db0f6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bbdd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    # \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = [\"streamflow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e512a383",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = ProcessingConfig(\n",
    "    required_columns=forcing_features + target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0961ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ca = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"USA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_ca = CaravanifyParquet(config_ca)\n",
    "basin_ids = caravan_ca.get_all_gauge_ids()[:250]\n",
    "# basin_ids = [\"CA_15030\"]\n",
    "\n",
    "# basin_ids = [bid for bid in basin_ids if bid != \"CA_15030\"]\n",
    "\n",
    "caravan_ca.load_stations(basin_ids)\n",
    "\n",
    "time_series = caravan_ca.get_time_series()[\n",
    "    forcing_features + [\"date\", \"gauge_id\"] + target\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30074ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(time_series)\n",
    "time_series = pl.from_pandas(time_series).lazy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee6c16",
   "metadata": {},
   "source": [
    "## Testing the `clean_data` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ade58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = clean_data(time_series, configs)\n",
    "\n",
    "if isinstance(result, Success):\n",
    "    cleaned_df, quality_report = result.unwrap()\n",
    "else:\n",
    "    error_msg = result.failure() \n",
    "    print(\"Data cleaning failed:\", error_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d866f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/Users/cooper/Desktop/hydro-forecasting/tests\"\n",
    "\n",
    "\n",
    "for gauge_id, report in quality_report.items():\n",
    "    succs_flag, path, error = save_quality_report_to_json(\n",
    "        report=report,\n",
    "        path=f\"/Users/cooper/Desktop/hydro-forecasting/tests/{gauge_id[0]}.json\",\n",
    "    )\n",
    "\n",
    "    if succs_flag:\n",
    "        print(f\"Quality report saved to {path}\")\n",
    "    else:\n",
    "        print(f\"Failed to save quality report: {error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3782c02e",
   "metadata": {},
   "source": [
    "## Testing the pipeline fitting and transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0457cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    "    chunk_size=50,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    "    chunk_size=50,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d32e720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert cleaned_df back to a pandas DataFrame\n",
    "cleaned_df = cleaned_df.to_pandas()\n",
    "\n",
    "result = fit_time_series_pipelines(\n",
    "    cleaned_df, \n",
    "    features_pipeline = feature_pipeline,\n",
    "    target_pipeline = target_pipeline,\n",
    ")\n",
    "\n",
    "if isinstance(result, Success):\n",
    "    fitted_pipelines = result.unwrap()\n",
    "else:\n",
    "    error_msg = result.failure() \n",
    "    print(\"Fitting time series pipelines failed:\", error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648d9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25841740",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = transform_time_series_data(\n",
    "    cleaned_df,\n",
    "    fitted_pipelines\n",
    ")\n",
    "\n",
    "if isinstance(result, Success):\n",
    "    transformed_df = result.unwrap()\n",
    "else:\n",
    "    error_msg = result.failure() \n",
    "    print(\"Transforming time series data failed:\", error_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b756d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_transformation(df, columns):\n",
    "    for column in columns:\n",
    "        # Make sure mean = 0 and std = 1\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        if not (abs(mean) < 1e-6 and abs(std - 1) < 1e-6):\n",
    "            print(f\"Column {column} failed validation: mean = {mean}, std = {std}\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "validation = validate_transformation(\n",
    "    transformed_df,\n",
    "    target + forcing_features,\n",
    ")\n",
    "\n",
    "if validation:\n",
    "    print(\"All columns passed validation.\")\n",
    "else:\n",
    "    print(\"Some columns failed validation.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829427c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save, _, error = save_time_series_pipelines(\n",
    "    fitted_pipelines,\n",
    "    \"/Users/cooper/Desktop/hydro-forecasting/tests/fitted_pipelines.joblib\"\n",
    ")\n",
    "\n",
    "if error:\n",
    "    print(\"Error saving pipelines:\", error)\n",
    "else:\n",
    "    print(\"Pipelines saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73293aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load, pipelines, error = load_time_series_pipelines(\n",
    "    \"/Users/cooper/Desktop/hydro-forecasting/tests/fitted_pipelines.joblib\"\n",
    ")\n",
    "\n",
    "if error:\n",
    "    print(\"Error loading pipelines:\", error)\n",
    "else:\n",
    "    print(\"Pipelines loaded successfully.\")\n",
    "    print(\"Loaded pipelines:\", load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71f8e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
