{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ed7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/cooper/Desktop/hydro-forecasting/src to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent  \n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b229203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from hydro_forecasting.data.caravanify import Caravanify, CaravanifyConfig\n",
    "from hydro_forecasting.data.caravanify_parquet import CaravanifyParquet, CaravanifyParquetConfig\n",
    "from hydro_forecasting.data.preprocessing import split_data, Config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21f29d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a792945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     276.198974\n",
       "1     103.274219\n",
       "2     174.744464\n",
       "3     303.634479\n",
       "4     378.958111\n",
       "5     644.379447\n",
       "6     626.000000\n",
       "7     548.306359\n",
       "8     282.385672\n",
       "9      43.392693\n",
       "10    369.000000\n",
       "11     62.022910\n",
       "12    187.342696\n",
       "13    140.423908\n",
       "14    527.042664\n",
       "15    739.598304\n",
       "16    422.189310\n",
       "17    463.621079\n",
       "18    456.324693\n",
       "19    450.489605\n",
       "Name: ele_mt_sav, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "    # human_influence_path=\"/Users/cooper/Desktop/CAMELS-CH/src/human_influence_index/results/human_influence_classification.csv\",\n",
    "    gauge_id_prefix=\"USA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan = CaravanifyParquet(config)\n",
    "basins = caravan.get_all_gauge_ids()[:20]\n",
    "\n",
    "caravan.load_stations(basins)\n",
    "\n",
    "static = caravan.get_static_attributes()\n",
    "static[\"ele_mt_sav\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd645f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb91a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "    # human_influence_path=\"/Users/cooper/Desktop/CAMELS-CH/src/human_influence_index/results/human_influence_classification.csv\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan = Caravanify(config)\n",
    "basins = caravan.get_all_gauge_ids()\n",
    "basins = [basin for basin in basins if basin!=\"CA_15030\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081cbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gauge_parquet(gauge_ids: list[str], base_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the .parquet file for a given list of gauge_ids.\n",
    "\n",
    "    Args:\n",
    "        gauge_ids (list[str]): Gauge IDs with the 'USA_' prefix.\n",
    "        base_dir (Path): Path to the directory containing the parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined data from the corresponding parquet files.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for gauge_id in gauge_ids:\n",
    "        file_path = base_dir / f\"{gauge_id}.parquet\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"No parquet file found for gauge ID {gauge_id} at {file_path}\"\n",
    "            )\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            df[\"gauge_id\"] = gauge_id  # Assign here\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    combined_data = pd.concat(data, ignore_index=True)\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7799cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 77/77 [00:00<00:00, 83.19it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_split_boundaries(train, val, test, gauge_ids):\n",
    "    \"\"\"\n",
    "    Determine the date boundaries between train/val/test splits for each gauge ID.\n",
    "\n",
    "    Args:\n",
    "        train: Training DataFrame\n",
    "        val: Validation DataFrame\n",
    "        test: Test DataFrame\n",
    "        gauge_ids: List of gauge IDs to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping gauge_ids to their split boundary dates\n",
    "    \"\"\"\n",
    "    split_boundaries = {}\n",
    "\n",
    "    for gauge_id in gauge_ids:\n",
    "        # Get min dates for val and test splits for this gauge\n",
    "        gauge_val = val[val[\"gauge_id\"] == gauge_id]\n",
    "        gauge_test = test[test[\"gauge_id\"] == gauge_id]\n",
    "\n",
    "        val_start = gauge_val[\"date\"].min() if not gauge_val.empty else None\n",
    "        test_start = gauge_test[\"date\"].min() if not gauge_test.empty else None\n",
    "\n",
    "        split_boundaries[gauge_id] = {\"val_start\": val_start, \"test_start\": test_start}\n",
    "\n",
    "    return split_boundaries\n",
    "\n",
    "\n",
    "def find_valid_sequences(basin_data, input_length, output_length, cols_to_check=None):\n",
    "    \"\"\"\n",
    "    Find valid sequence starting positions in the basin data.\n",
    "\n",
    "    Args:\n",
    "        basin_data: DataFrame containing basin time series data\n",
    "        input_length: Length of input sequence\n",
    "        output_length: Length of output sequence\n",
    "        cols_to_check: Columns to check for NaN values\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (valid_positions, dates) arrays\n",
    "    \"\"\"\n",
    "    if cols_to_check is None:\n",
    "        cols_to_check = [\"streamflow\", \"total_precipitation_sum\"]\n",
    "\n",
    "    total_seq_length = input_length + output_length\n",
    "\n",
    "    if len(basin_data) < total_seq_length:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Extract needed data as arrays\n",
    "    basin_values = basin_data[cols_to_check].to_numpy()\n",
    "    dates = basin_data[\"date\"].to_numpy()\n",
    "\n",
    "    # Combined valid mask: 1 if all cols not NaN, 0 otherwise\n",
    "    combined_valid = (~np.isnan(basin_values).any(axis=1)).astype(int)\n",
    "\n",
    "    # Convolve to find valid input sequences\n",
    "    input_conv = np.convolve(\n",
    "        combined_valid, np.ones(input_length, dtype=int), mode=\"valid\"\n",
    "    )\n",
    "    input_valid = input_conv == input_length\n",
    "\n",
    "    # Convolve for output sequences, shifted by input_length\n",
    "    output_conv = np.convolve(\n",
    "        combined_valid, np.ones(output_length, dtype=int), mode=\"valid\"\n",
    "    )\n",
    "    output_valid = output_conv == output_length\n",
    "    output_valid_shifted = np.pad(\n",
    "        output_valid, (input_length, 0), constant_values=False\n",
    "    )[: len(input_valid)]\n",
    "\n",
    "    # Find valid sequence starts\n",
    "    valid_mask = input_valid & output_valid_shifted\n",
    "    valid_positions = np.where(valid_mask)[0]\n",
    "\n",
    "    return valid_positions, dates\n",
    "\n",
    "\n",
    "def determine_stage(input_end_date, boundaries):\n",
    "    \"\"\"\n",
    "    Determine which stage (train/val/test) a sequence belongs to based on its end date.\n",
    "\n",
    "    Args:\n",
    "        input_end_date: End date of the input sequence\n",
    "        boundaries: Dictionary with val_start and test_start dates\n",
    "\n",
    "    Returns:\n",
    "        String: 'train', 'val', or 'test'\n",
    "    \"\"\"\n",
    "    val_start = boundaries[\"val_start\"]\n",
    "    test_start = boundaries[\"test_start\"]\n",
    "\n",
    "    if test_start is not None and input_end_date >= test_start:\n",
    "        return \"test\"\n",
    "    elif val_start is not None and input_end_date >= val_start:\n",
    "        return \"val\"\n",
    "    else:\n",
    "        return \"train\"\n",
    "\n",
    "\n",
    "def create_basin_index(\n",
    "    gauge_ids: list[str],\n",
    "    base_dir: Path,\n",
    "    static_file_path: Path,\n",
    "    input_length=70,\n",
    "    output_length=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create index entries for valid sequences, identifying which stage (train/val/test) each sequence belongs to.\n",
    "\n",
    "    Args:\n",
    "        gauge_ids: List of gauge IDs to process\n",
    "        base_dir: Base directory containing parquet files\n",
    "        static_file_path: Path to static attributes file\n",
    "        input_length: Length of input sequence\n",
    "        output_length: Length of forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        List of index entries with stage identification\n",
    "    \"\"\"\n",
    "    valid_data = load_gauge_parquet(gauge_ids, base_dir)\n",
    "\n",
    "    # Create splits\n",
    "    split_config = Config(\n",
    "        required_columns=[\"streamflow\"],\n",
    "        preprocessing_config={},\n",
    "        min_train_years=10,\n",
    "        max_imputation_gap_size=5,\n",
    "    )\n",
    "    train, val, test = split_data(df=valid_data, config=split_config)\n",
    "\n",
    "    # Get split boundaries for each gauge\n",
    "    split_boundaries = get_split_boundaries(train, val, test, gauge_ids)\n",
    "\n",
    "    all_index_entries = []\n",
    "    total_seq_length = input_length + output_length\n",
    "\n",
    "    # Process each basin\n",
    "    for gauge_id, basin_data in tqdm(\n",
    "        valid_data.groupby(\"gauge_id\"), desc=\"Processing basins\"\n",
    "    ):\n",
    "        # Create actual file path for this gauge\n",
    "        ts_file_path = base_dir / f\"{gauge_id}.parquet\"\n",
    "\n",
    "        # Get split boundaries for this gauge\n",
    "        gauge_bounds = split_boundaries.get(\n",
    "            gauge_id, {\"val_start\": None, \"test_start\": None}\n",
    "        )\n",
    "\n",
    "        # Find valid sequences in this basin's data\n",
    "        valid_positions, dates = find_valid_sequences(\n",
    "            basin_data, input_length, output_length\n",
    "        )\n",
    "\n",
    "        # Create index entries with stage identification\n",
    "        for idx in valid_positions:\n",
    "            if idx + total_seq_length > len(basin_data):\n",
    "                continue\n",
    "\n",
    "            # Get the input_end_date for this sequence\n",
    "            input_end_date = dates[idx + input_length - 1]\n",
    "\n",
    "            # Determine stage based on input_end_date\n",
    "            stage = determine_stage(input_end_date, gauge_bounds)\n",
    "\n",
    "            # Create entry with stage information\n",
    "            entry = {\n",
    "                \"file_path\": str(ts_file_path),\n",
    "                \"static_file_path\": str(static_file_path),\n",
    "                \"gauge_id\": gauge_id,\n",
    "                \"start_idx\": idx,\n",
    "                \"end_idx\": idx + total_seq_length,\n",
    "                \"input_end_date\": input_end_date,\n",
    "                \"valid_sequence\": True,\n",
    "                \"stage\": stage,\n",
    "            }\n",
    "\n",
    "            all_index_entries.append(entry)\n",
    "\n",
    "    return all_index_entries\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "data_folder = Path(\n",
    "    \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_data\"\n",
    ")\n",
    "static_file = Path(\"/path/to/static_attributes.csv\")\n",
    "index_entries = create_basin_index(basins, data_folder, static_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a69e4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ranges:   0%|          | 0/2048 [00:00<?, ?it/s]/var/folders/y6/kqwqph4s3sj7hkxryrly5y6r0000gn/T/ipykernel_4739/1238219964.py:28: DeprecationWarning: The argument `row_count_name` for `read_parquet` is deprecated. It has been renamed to `row_index_name`.\n",
      "  return pl.read_parquet(\n",
      "Reading ranges: 100%|██████████| 2048/2048 [00:00<00:00, 2807.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read ranges: 0.73104s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def read_parquet_range(file_path: str, start_idx: int, end_idx: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Efficiently read a row slice from a Parquet file using Polars.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the Parquet file.\n",
    "        start_idx: Start row index (inclusive).\n",
    "        end_idx: End row index (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with the selected rows.\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        \"date\",\n",
    "        \"snow_depth_water_equivalent_mean\",\n",
    "        \"surface_net_solar_radiation_mean\",\n",
    "        \"surface_net_thermal_radiation_mean\",\n",
    "        \"potential_evaporation_sum_ERA5_LAND\",\n",
    "        \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"temperature_2m_min\",\n",
    "        \"temperature_2m_max\",\n",
    "        \"total_precipitation_sum\",\n",
    "    ]\n",
    "    return pl.read_parquet(\n",
    "        file_path, columns=columns, row_count_name=None, use_pyarrow=False\n",
    "    ).slice(start_idx, end_idx - start_idx)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "random_indices = random.sample(range(len(index_entries)), 2048)\n",
    "dfs = []\n",
    "start = time.time()\n",
    "for i in tqdm(random_indices, desc=\"Reading ranges\"):\n",
    "    df = read_parquet_range(\n",
    "        file_path=index_entries[i][\"file_path\"],\n",
    "        start_idx=index_entries[i][\"start_idx\"],\n",
    "        end_idx=index_entries[i][\"end_idx\"],\n",
    "    )\n",
    "\n",
    "    dfs.append(df)\n",
    "    del df\n",
    "print(f\"Time taken to read ranges: {time.time() - start:.5f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3907d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_data/CA_15013.parquet',\n",
       " 'static_file_path': '/path/to/static_attributes.csv',\n",
       " 'gauge_id': 'CA_15013',\n",
       " 'start_idx': np.int64(70),\n",
       " 'end_idx': np.int64(150),\n",
       " 'input_end_date': np.datetime64('2000-05-20T00:00:00.000000000'),\n",
       " 'valid_sequence': True,\n",
       " 'stage': 'train'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00427d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (80, 10)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>snow_depth_water_equivalent_mean</th><th>surface_net_solar_radiation_mean</th><th>surface_net_thermal_radiation_mean</th><th>potential_evaporation_sum_ERA5_LAND</th><th>potential_evaporation_sum_FAO_PENMAN_MONTEITH</th><th>temperature_2m_mean</th><th>temperature_2m_min</th><th>temperature_2m_max</th><th>total_precipitation_sum</th></tr><tr><td>datetime[ns]</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>2015-09-07 00:00:00</td><td>0.9</td><td>205.789993</td><td>-92.57</td><td>1.260514</td><td>2.08</td><td>2.02</td><td>-2.43</td><td>6.56</td><td>-0.666093</td></tr><tr><td>2015-09-08 00:00:00</td><td>0.34</td><td>221.889999</td><td>-107.910004</td><td>1.332716</td><td>2.22</td><td>3.29</td><td>-1.7</td><td>8.43</td><td>-0.739411</td></tr><tr><td>2015-09-09 00:00:00</td><td>0.11</td><td>223.029999</td><td>-107.199997</td><td>1.47712</td><td>2.47</td><td>5.93</td><td>0.65</td><td>11.43</td><td>-0.743855</td></tr><tr><td>2015-09-10 00:00:00</td><td>0.03</td><td>219.929993</td><td>-103.150002</td><td>1.482674</td><td>2.66</td><td>8.42</td><td>2.78</td><td>14.2</td><td>-0.641654</td></tr><tr><td>2015-09-11 00:00:00</td><td>0.01</td><td>200.789993</td><td>-89.199997</td><td>1.493782</td><td>2.59</td><td>8.34</td><td>3.49</td><td>13.36</td><td>-0.488354</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2015-11-21 00:00:00</td><td>178.479996</td><td>17.48</td><td>-29.549999</td><td>-0.788909</td><td>0.0</td><td>-8.07</td><td>-9.67</td><td>-5.56</td><td>-0.41948</td></tr><tr><td>2015-11-22 00:00:00</td><td>180.110001</td><td>15.24</td><td>-25.950001</td><td>-0.788909</td><td>0.0</td><td>-7.85</td><td>-11.75</td><td>-5.18</td><td>-0.463914</td></tr><tr><td>2015-11-23 00:00:00</td><td>180.429993</td><td>20.290001</td><td>-49.540001</td><td>-0.777801</td><td>0.0</td><td>-10.34</td><td>-15.99</td><td>-5.18</td><td>-0.743855</td></tr><tr><td>2015-11-24 00:00:00</td><td>180.440002</td><td>21.9</td><td>-52.139999</td><td>-0.777801</td><td>0.0</td><td>-8.09</td><td>-10.16</td><td>-5.51</td><td>-0.743855</td></tr><tr><td>2015-11-25 00:00:00</td><td>180.449997</td><td>22.0</td><td>-36.369999</td><td>-0.777801</td><td>0.12</td><td>-8.59</td><td>-10.18</td><td>-6.75</td><td>-0.743855</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (80, 10)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ date      ┆ snow_dept ┆ surface_n ┆ surface_n ┆ … ┆ temperatu ┆ temperatu ┆ temperatu ┆ total_pr │\n",
       "│ ---       ┆ h_water_e ┆ et_solar_ ┆ et_therma ┆   ┆ re_2m_mea ┆ re_2m_min ┆ re_2m_max ┆ ecipitat │\n",
       "│ datetime[ ┆ quivalent ┆ radiation ┆ l_radiati ┆   ┆ n         ┆ ---       ┆ ---       ┆ ion_sum  │\n",
       "│ ns]       ┆ _me…      ┆ _me…      ┆ on_…      ┆   ┆ ---       ┆ f32       ┆ f32       ┆ ---      │\n",
       "│           ┆ ---       ┆ ---       ┆ ---       ┆   ┆ f32       ┆           ┆           ┆ f32      │\n",
       "│           ┆ f32       ┆ f32       ┆ f32       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2015-09-0 ┆ 0.9       ┆ 205.78999 ┆ -92.57    ┆ … ┆ 2.02      ┆ -2.43     ┆ 6.56      ┆ -0.66609 │\n",
       "│ 7         ┆           ┆ 3         ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-09-0 ┆ 0.34      ┆ 221.88999 ┆ -107.9100 ┆ … ┆ 3.29      ┆ -1.7      ┆ 8.43      ┆ -0.73941 │\n",
       "│ 8         ┆           ┆ 9         ┆ 04        ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-09-0 ┆ 0.11      ┆ 223.02999 ┆ -107.1999 ┆ … ┆ 5.93      ┆ 0.65      ┆ 11.43     ┆ -0.74385 │\n",
       "│ 9         ┆           ┆ 9         ┆ 97        ┆   ┆           ┆           ┆           ┆ 5        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-09-1 ┆ 0.03      ┆ 219.92999 ┆ -103.1500 ┆ … ┆ 8.42      ┆ 2.78      ┆ 14.2      ┆ -0.64165 │\n",
       "│ 0         ┆           ┆ 3         ┆ 02        ┆   ┆           ┆           ┆           ┆ 4        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-09-1 ┆ 0.01      ┆ 200.78999 ┆ -89.19999 ┆ … ┆ 8.34      ┆ 3.49      ┆ 13.36     ┆ -0.48835 │\n",
       "│ 1         ┆           ┆ 3         ┆ 7         ┆   ┆           ┆           ┆           ┆ 4        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 2015-11-2 ┆ 178.47999 ┆ 17.48     ┆ -29.54999 ┆ … ┆ -8.07     ┆ -9.67     ┆ -5.56     ┆ -0.41948 │\n",
       "│ 1         ┆ 6         ┆           ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-11-2 ┆ 180.11000 ┆ 15.24     ┆ -25.95000 ┆ … ┆ -7.85     ┆ -11.75    ┆ -5.18     ┆ -0.46391 │\n",
       "│ 2         ┆ 1         ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆ 4        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-11-2 ┆ 180.42999 ┆ 20.290001 ┆ -49.54000 ┆ … ┆ -10.34    ┆ -15.99    ┆ -5.18     ┆ -0.74385 │\n",
       "│ 3         ┆ 3         ┆           ┆ 1         ┆   ┆           ┆           ┆           ┆ 5        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-11-2 ┆ 180.44000 ┆ 21.9      ┆ -52.13999 ┆ … ┆ -8.09     ┆ -10.16    ┆ -5.51     ┆ -0.74385 │\n",
       "│ 4         ┆ 2         ┆           ┆ 9         ┆   ┆           ┆           ┆           ┆ 5        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2015-11-2 ┆ 180.44999 ┆ 22.0      ┆ -36.36999 ┆ … ┆ -8.59     ┆ -10.18    ┆ -6.75     ┆ -0.74385 │\n",
       "│ 5         ┆ 7         ┆           ┆ 9         ┆   ┆           ┆           ┆           ┆ 5        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6c692",
   "metadata": {},
   "source": [
    "## Splitting index entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce1d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_index_entreis_by_stage(\n",
    "    index_entries: list[dict],\n",
    ") -> dict[str, list[dict]]:\n",
    "    \"\"\"\n",
    "    Split index entries into train, val, and test sets based on their stage.\n",
    "\n",
    "    Args:\n",
    "        index_entries: List of index entries with stage information\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys 'train', 'val', and 'test' mapping to lists of index entries\n",
    "    \"\"\"\n",
    "    split_entries = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "    for entry in index_entries:\n",
    "        stage = entry[\"stage\"]\n",
    "        if stage in split_entries:\n",
    "            split_entries[stage].append(entry)\n",
    "\n",
    "    return split_entries\n",
    "\n",
    "# Example usage\n",
    "split_entries = split_index_entreis_by_stage(index_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d51c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb0c8a0",
   "metadata": {},
   "source": [
    "## Dealing with static attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821bd9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read static attributes: 0.79953s\n"
     ]
    }
   ],
   "source": [
    "path_to_static = Path(\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_static_data/static_attributes.parquet\")\n",
    "\n",
    "def get_static_attributes_from_id(path, id):\n",
    "    static_df = pd.read_parquet(path_to_static)\n",
    "    static_df = static_df[static_df[\"gauge_id\"] == id]\n",
    "    return static_df\n",
    "\n",
    "statics = []\n",
    "start = time.time()\n",
    "for id in basins:\n",
    "    static = get_static_attributes_from_id(path_to_static, id)\n",
    "    statics.append(static)\n",
    "    del static\n",
    "end = time.time()\n",
    "print(f\"Time taken to read static attributes: {end - start:.5f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c2fe66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read static attributes: 0.06610s\n"
     ]
    }
   ],
   "source": [
    "def get_static_attributes_from_id(\n",
    "    static_df: pl.DataFrame, gauge_id: str, static_columns: list[str]\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve static attributes for a specific gauge ID from a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        static_df: Polars DataFrame containing static attributes for all gauges.\n",
    "        gauge_id: The gauge ID to filter for.\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with static attributes for the specified gauge.\n",
    "    \"\"\"\n",
    "    static_df = pl.read_parquet(str(path_to_static), columns=static_columns)\n",
    "    filtered_df = static_df.filter(pl.col(\"gauge_id\") == gauge_id)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def get_all_statics_for_basins(\n",
    "    path_to_static: Path, basin_ids: list[str], static_columns: list[str] = None\n",
    ") -> list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Efficiently retrieve static attributes for a list of basin IDs.\n",
    "\n",
    "    Args:\n",
    "        path_to_static: Path to the static attributes parquet file.\n",
    "        basin_ids: List of basin (gauge) IDs.\n",
    "\n",
    "    Returns:\n",
    "        List of Polars DataFrames, one per basin ID.\n",
    "\n",
    "    Example:\n",
    "        >>> statics = get_all_statics_for_basins(Path(\"static_attributes.parquet\"), [\"CA-001\", \"CA-002\"])\n",
    "    \"\"\"\n",
    "    static_df = pl.read_parquet(path_to_static)\n",
    "    statics = []\n",
    "    start = time.time()\n",
    "    for gauge_id in basin_ids:\n",
    "        statics.append(\n",
    "            get_static_attributes_from_id(static_df, gauge_id, static_columns)\n",
    "        )\n",
    "    end = time.time()\n",
    "    print(f\"Time taken to read static attributes: {end - start:.5f}s\")\n",
    "    return statics\n",
    "\n",
    "\n",
    "static_cols = [\n",
    "    \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "statics = get_all_statics_for_basins(path_to_static, basins, static_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c086d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(statics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145f9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
