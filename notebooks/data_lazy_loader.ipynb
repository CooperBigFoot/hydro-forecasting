{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ed7918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/cooper/Desktop/hydro-forecasting/src to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b229203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from hydro_forecasting.data.caravanify import Caravanify, CaravanifyConfig\n",
    "from hydro_forecasting.data.caravanify_parquet import CaravanifyParquet, CaravanifyParquetConfig\n",
    "from hydro_forecasting.data.preprocessing import Config, split_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21f29d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a792945c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     276.198974\n",
       "1     103.274219\n",
       "2     174.744464\n",
       "3     303.634479\n",
       "4     378.958111\n",
       "5     644.379447\n",
       "6     626.000000\n",
       "7     548.306359\n",
       "8     282.385672\n",
       "9      43.392693\n",
       "10    369.000000\n",
       "11     62.022910\n",
       "12    187.342696\n",
       "13    140.423908\n",
       "14    527.042664\n",
       "15    739.598304\n",
       "16    422.189310\n",
       "17    463.621079\n",
       "18    456.324693\n",
       "19    450.489605\n",
       "Name: ele_mt_sav, dtype: float64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "    # human_influence_path=\"/Users/cooper/Desktop/CAMELS-CH/src/human_influence_index/results/human_influence_classification.csv\",\n",
    "    gauge_id_prefix=\"USA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan = CaravanifyParquet(config)\n",
    "basins = caravan.get_all_gauge_ids()[:20]\n",
    "\n",
    "caravan.load_stations(basins)\n",
    "\n",
    "static = caravan.get_static_attributes()\n",
    "static[\"ele_mt_sav\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd645f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb91a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = CaravanifyConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "    # human_influence_path=\"/Users/cooper/Desktop/CAMELS-CH/src/human_influence_index/results/human_influence_classification.csv\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan = Caravanify(config)\n",
    "basins = caravan.get_all_gauge_ids()\n",
    "basins = [basin for basin in basins if basin != \"CA_15030\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "081cbdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gauge_parquet(gauge_ids: list[str], base_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the .parquet file for a given list of gauge_ids.\n",
    "\n",
    "    Args:\n",
    "        gauge_ids (list[str]): Gauge IDs with the 'USA_' prefix.\n",
    "        base_dir (Path): Path to the directory containing the parquet files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined data from the corresponding parquet files.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for gauge_id in gauge_ids:\n",
    "        file_path = base_dir / f\"{gauge_id}.parquet\"\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"No parquet file found for gauge ID {gauge_id} at {file_path}\")\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "            df[\"gauge_id\"] = gauge_id  # Assign here\n",
    "            data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    combined_data = pd.concat(data, ignore_index=True)\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7799cb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 77/77 [00:00<00:00, 84.93it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_split_boundaries(train, val, test, gauge_ids):\n",
    "    \"\"\"\n",
    "    Determine the date boundaries between train/val/test splits for each gauge ID.\n",
    "\n",
    "    Args:\n",
    "        train: Training DataFrame\n",
    "        val: Validation DataFrame\n",
    "        test: Test DataFrame\n",
    "        gauge_ids: List of gauge IDs to process\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping gauge_ids to their split boundary dates\n",
    "    \"\"\"\n",
    "    split_boundaries = {}\n",
    "\n",
    "    for gauge_id in gauge_ids:\n",
    "        # Get min dates for val and test splits for this gauge\n",
    "        gauge_val = val[val[\"gauge_id\"] == gauge_id]\n",
    "        gauge_test = test[test[\"gauge_id\"] == gauge_id]\n",
    "\n",
    "        val_start = gauge_val[\"date\"].min() if not gauge_val.empty else None\n",
    "        test_start = gauge_test[\"date\"].min() if not gauge_test.empty else None\n",
    "\n",
    "        split_boundaries[gauge_id] = {\"val_start\": val_start, \"test_start\": test_start}\n",
    "\n",
    "    return split_boundaries\n",
    "\n",
    "\n",
    "def find_valid_sequences(basin_data, input_length, output_length, cols_to_check=None):\n",
    "    \"\"\"\n",
    "    Find valid sequence starting positions in the basin data.\n",
    "\n",
    "    Args:\n",
    "        basin_data: DataFrame containing basin time series data\n",
    "        input_length: Length of input sequence\n",
    "        output_length: Length of output sequence\n",
    "        cols_to_check: Columns to check for NaN values\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (valid_positions, dates) arrays\n",
    "    \"\"\"\n",
    "    if cols_to_check is None:\n",
    "        cols_to_check = [\"streamflow\", \"total_precipitation_sum\"]\n",
    "\n",
    "    total_seq_length = input_length + output_length\n",
    "\n",
    "    if len(basin_data) < total_seq_length:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Extract needed data as arrays\n",
    "    basin_values = basin_data[cols_to_check].to_numpy()\n",
    "    dates = basin_data[\"date\"].to_numpy()\n",
    "\n",
    "    # Combined valid mask: 1 if all cols not NaN, 0 otherwise\n",
    "    combined_valid = (~np.isnan(basin_values).any(axis=1)).astype(int)\n",
    "\n",
    "    # Convolve to find valid input sequences\n",
    "    input_conv = np.convolve(combined_valid, np.ones(input_length, dtype=int), mode=\"valid\")\n",
    "    input_valid = input_conv == input_length\n",
    "\n",
    "    # Convolve for output sequences, shifted by input_length\n",
    "    output_conv = np.convolve(combined_valid, np.ones(output_length, dtype=int), mode=\"valid\")\n",
    "    output_valid = output_conv == output_length\n",
    "    output_valid_shifted = np.pad(output_valid, (input_length, 0), constant_values=False)[: len(input_valid)]\n",
    "\n",
    "    # Find valid sequence starts\n",
    "    valid_mask = input_valid & output_valid_shifted\n",
    "    valid_positions = np.where(valid_mask)[0]\n",
    "\n",
    "    return valid_positions, dates\n",
    "\n",
    "\n",
    "def determine_stage(input_end_date, boundaries):\n",
    "    \"\"\"\n",
    "    Determine which stage (train/val/test) a sequence belongs to based on its end date.\n",
    "\n",
    "    Args:\n",
    "        input_end_date: End date of the input sequence\n",
    "        boundaries: Dictionary with val_start and test_start dates\n",
    "\n",
    "    Returns:\n",
    "        String: 'train', 'val', or 'test'\n",
    "    \"\"\"\n",
    "    val_start = boundaries[\"val_start\"]\n",
    "    test_start = boundaries[\"test_start\"]\n",
    "\n",
    "    if test_start is not None and input_end_date >= test_start:\n",
    "        return \"test\"\n",
    "    elif val_start is not None and input_end_date >= val_start:\n",
    "        return \"val\"\n",
    "    else:\n",
    "        return \"train\"\n",
    "\n",
    "\n",
    "def create_basin_index(\n",
    "    gauge_ids: list[str],\n",
    "    base_dir: Path,\n",
    "    static_file_path: Path,\n",
    "    input_length=70,\n",
    "    output_length=10,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create index entries for valid sequences, identifying which stage (train/val/test) each sequence belongs to.\n",
    "\n",
    "    Args:\n",
    "        gauge_ids: List of gauge IDs to process\n",
    "        base_dir: Base directory containing parquet files\n",
    "        static_file_path: Path to static attributes file\n",
    "        input_length: Length of input sequence\n",
    "        output_length: Length of forecast horizon\n",
    "\n",
    "    Returns:\n",
    "        List of index entries with stage identification\n",
    "    \"\"\"\n",
    "    valid_data = load_gauge_parquet(gauge_ids, base_dir)\n",
    "\n",
    "    # Create splits\n",
    "    split_config = Config(\n",
    "        required_columns=[\"streamflow\"],\n",
    "        preprocessing_config={},\n",
    "        min_train_years=10,\n",
    "        max_imputation_gap_size=5,\n",
    "    )\n",
    "    train, val, test = split_data(df=valid_data, config=split_config)\n",
    "\n",
    "    # Get split boundaries for each gauge\n",
    "    split_boundaries = get_split_boundaries(train, val, test, gauge_ids)\n",
    "\n",
    "    all_index_entries = []\n",
    "    total_seq_length = input_length + output_length\n",
    "\n",
    "    # Process each basin\n",
    "    for gauge_id, basin_data in tqdm(valid_data.groupby(\"gauge_id\"), desc=\"Processing basins\"):\n",
    "        # Create actual file path for this gauge\n",
    "        ts_file_path = base_dir / f\"{gauge_id}.parquet\"\n",
    "\n",
    "        # Get split boundaries for this gauge\n",
    "        gauge_bounds = split_boundaries.get(gauge_id, {\"val_start\": None, \"test_start\": None})\n",
    "\n",
    "        # Find valid sequences in this basin's data\n",
    "        valid_positions, dates = find_valid_sequences(basin_data, input_length, output_length)\n",
    "\n",
    "        # Create index entries with stage identification\n",
    "        for idx in valid_positions:\n",
    "            if idx + total_seq_length > len(basin_data):\n",
    "                continue\n",
    "\n",
    "            # Get the input_end_date for this sequence\n",
    "            input_end_date = dates[idx + input_length - 1]\n",
    "\n",
    "            # Determine stage based on input_end_date\n",
    "            stage = determine_stage(input_end_date, gauge_bounds)\n",
    "\n",
    "            # Create entry with stage information\n",
    "            entry = {\n",
    "                \"file_path\": str(ts_file_path),\n",
    "                \"static_file_path\": str(static_file_path),\n",
    "                \"gauge_id\": gauge_id,\n",
    "                \"start_idx\": idx,\n",
    "                \"end_idx\": idx + total_seq_length,\n",
    "                \"input_end_date\": input_end_date,\n",
    "                \"valid_sequence\": True,\n",
    "                \"stage\": stage,\n",
    "            }\n",
    "\n",
    "            all_index_entries.append(entry)\n",
    "\n",
    "    return all_index_entries\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "data_folder = Path(\n",
    "    \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_data\"\n",
    ")\n",
    "static_file = Path(\"/path/to/static_attributes.csv\")\n",
    "index_entries = create_basin_index(basins, data_folder, static_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69e4c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ranges:   0%|          | 0/2048 [00:00<?, ?it/s]/var/folders/y6/kqwqph4s3sj7hkxryrly5y6r0000gn/T/ipykernel_19052/601109034.py:29: DeprecationWarning: The argument `row_count_name` for `read_parquet` is deprecated. It has been renamed to `row_index_name`.\n",
      "  return pl.read_parquet(\n",
      "Reading ranges: 100%|██████████| 2048/2048 [00:00<00:00, 2322.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read ranges: 0.88313s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def read_parquet_range(file_path: str, start_idx: int, end_idx: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Efficiently read a row slice from a Parquet file using Polars.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the Parquet file.\n",
    "        start_idx: Start row index (inclusive).\n",
    "        end_idx: End row index (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with the selected rows.\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        \"date\",\n",
    "        \"streamflow\",\n",
    "        \"snow_depth_water_equivalent_mean\",\n",
    "        \"surface_net_solar_radiation_mean\",\n",
    "        \"surface_net_thermal_radiation_mean\",\n",
    "        \"potential_evaporation_sum_ERA5_LAND\",\n",
    "        \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "        \"temperature_2m_mean\",\n",
    "        \"temperature_2m_min\",\n",
    "        \"temperature_2m_max\",\n",
    "        \"total_precipitation_sum\",\n",
    "    ]\n",
    "    return pl.read_parquet(file_path, columns=columns, row_count_name=None, use_pyarrow=False).slice(\n",
    "        start_idx, end_idx - start_idx\n",
    "    )\n",
    "\n",
    "\n",
    "# Example usage\n",
    "random_indices = random.sample(range(len(index_entries)), 2048)\n",
    "dfs = []\n",
    "start = time.time()\n",
    "for i in tqdm(random_indices, desc=\"Reading ranges\"):\n",
    "    df = read_parquet_range(\n",
    "        file_path=index_entries[i][\"file_path\"],\n",
    "        start_idx=index_entries[i][\"start_idx\"],\n",
    "        end_idx=index_entries[i][\"end_idx\"],\n",
    "    )\n",
    "\n",
    "    dfs.append(df)\n",
    "    del df\n",
    "print(f\"Time taken to read ranges: {time.time() - start:.5f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3907d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': '/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_data/CA_15013.parquet',\n",
       " 'static_file_path': '/path/to/static_attributes.csv',\n",
       " 'gauge_id': 'CA_15013',\n",
       " 'start_idx': np.int64(70),\n",
       " 'end_idx': np.int64(150),\n",
       " 'input_end_date': np.datetime64('2000-05-20T00:00:00.000000000'),\n",
       " 'valid_sequence': True,\n",
       " 'stage': 'train'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_entries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00427d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (80, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date</th><th>streamflow</th><th>snow_depth_water_equivalent_mean</th><th>surface_net_solar_radiation_mean</th><th>surface_net_thermal_radiation_mean</th><th>potential_evaporation_sum_ERA5_LAND</th><th>potential_evaporation_sum_FAO_PENMAN_MONTEITH</th><th>temperature_2m_mean</th><th>temperature_2m_min</th><th>temperature_2m_max</th><th>total_precipitation_sum</th></tr><tr><td>datetime[ns]</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>2019-10-22 00:00:00</td><td>-0.442249</td><td>221.729996</td><td>100.93</td><td>-74.739998</td><td>-0.226614</td><td>0.48</td><td>-5.54</td><td>-9.3</td><td>-1.59</td><td>0.389522</td></tr><tr><td>2019-10-23 00:00:00</td><td>-0.398838</td><td>222.270004</td><td>117.529999</td><td>-95.989998</td><td>-0.149951</td><td>0.39</td><td>-7.44</td><td>-11.4</td><td>-1.68</td><td>-0.654873</td></tr><tr><td>2019-10-24 00:00:00</td><td>-0.442249</td><td>221.880005</td><td>127.089996</td><td>-110.260002</td><td>-0.085081</td><td>0.34</td><td>-6.97</td><td>-12.2</td><td>-0.56</td><td>-0.675271</td></tr><tr><td>2019-10-25 00:00:00</td><td>-0.463954</td><td>221.509995</td><td>130.270004</td><td>-115.809998</td><td>-0.067389</td><td>0.33</td><td>-7.28</td><td>-14.38</td><td>-0.15</td><td>-0.675271</td></tr><tr><td>2019-10-26 00:00:00</td><td>-0.463954</td><td>221.179993</td><td>128.919998</td><td>-110.620003</td><td>0.038761</td><td>0.48</td><td>-6.01</td><td>-12.76</td><td>0.61</td><td>-0.671192</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>2020-01-05 00:00:00</td><td>-0.724422</td><td>255.639999</td><td>41.389999</td><td>-47.349998</td><td>-0.928384</td><td>0.08</td><td>-19.700001</td><td>-27.0</td><td>-14.04</td><td>-0.475368</td></tr><tr><td>2020-01-06 00:00:00</td><td>-0.767834</td><td>257.290009</td><td>27.93</td><td>-27.25</td><td>-0.91659</td><td>0.1</td><td>-14.29</td><td>-17.309999</td><td>-10.6</td><td>0.316088</td></tr><tr><td>2020-01-07 00:00:00</td><td>-0.767834</td><td>258.570007</td><td>38.759998</td><td>-59.720001</td><td>-0.940178</td><td>0.0</td><td>-19.4</td><td>-25.01</td><td>-14.21</td><td>-0.548801</td></tr><tr><td>2020-01-08 00:00:00</td><td>-0.767834</td><td>258.630005</td><td>41.27</td><td>-59.540001</td><td>-0.940178</td><td>0.0</td><td>-23.98</td><td>-30.26</td><td>-16.98</td><td>-0.654873</td></tr><tr><td>2020-01-09 00:00:00</td><td>-0.767834</td><td>258.730011</td><td>33.669998</td><td>-36.34</td><td>-0.910692</td><td>0.09</td><td>-20.01</td><td>-27.389999</td><td>-13.49</td><td>-0.556961</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (80, 11)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ date      ┆ streamflo ┆ snow_dept ┆ surface_n ┆ … ┆ temperatu ┆ temperatu ┆ temperatu ┆ total_pr │\n",
       "│ ---       ┆ w         ┆ h_water_e ┆ et_solar_ ┆   ┆ re_2m_mea ┆ re_2m_min ┆ re_2m_max ┆ ecipitat │\n",
       "│ datetime[ ┆ ---       ┆ quivalent ┆ radiation ┆   ┆ n         ┆ ---       ┆ ---       ┆ ion_sum  │\n",
       "│ ns]       ┆ f32       ┆ _me…      ┆ _me…      ┆   ┆ ---       ┆ f32       ┆ f32       ┆ ---      │\n",
       "│           ┆           ┆ ---       ┆ ---       ┆   ┆ f32       ┆           ┆           ┆ f32      │\n",
       "│           ┆           ┆ f32       ┆ f32       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2019-10-2 ┆ -0.442249 ┆ 221.72999 ┆ 100.93    ┆ … ┆ -5.54     ┆ -9.3      ┆ -1.59     ┆ 0.389522 │\n",
       "│ 2         ┆           ┆ 6         ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2019-10-2 ┆ -0.398838 ┆ 222.27000 ┆ 117.52999 ┆ … ┆ -7.44     ┆ -11.4     ┆ -1.68     ┆ -0.65487 │\n",
       "│ 3         ┆           ┆ 4         ┆ 9         ┆   ┆           ┆           ┆           ┆ 3        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2019-10-2 ┆ -0.442249 ┆ 221.88000 ┆ 127.08999 ┆ … ┆ -6.97     ┆ -12.2     ┆ -0.56     ┆ -0.67527 │\n",
       "│ 4         ┆           ┆ 5         ┆ 6         ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2019-10-2 ┆ -0.463954 ┆ 221.50999 ┆ 130.27000 ┆ … ┆ -7.28     ┆ -14.38    ┆ -0.15     ┆ -0.67527 │\n",
       "│ 5         ┆           ┆ 5         ┆ 4         ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2019-10-2 ┆ -0.463954 ┆ 221.17999 ┆ 128.91999 ┆ … ┆ -6.01     ┆ -12.76    ┆ 0.61      ┆ -0.67119 │\n",
       "│ 6         ┆           ┆ 3         ┆ 8         ┆   ┆           ┆           ┆           ┆ 2        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ …         ┆ …         ┆ …         ┆ …         ┆ … ┆ …         ┆ …         ┆ …         ┆ …        │\n",
       "│ 2020-01-0 ┆ -0.724422 ┆ 255.63999 ┆ 41.389999 ┆ … ┆ -19.70000 ┆ -27.0     ┆ -14.04    ┆ -0.47536 │\n",
       "│ 5         ┆           ┆ 9         ┆           ┆   ┆ 1         ┆           ┆           ┆ 8        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2020-01-0 ┆ -0.767834 ┆ 257.29000 ┆ 27.93     ┆ … ┆ -14.29    ┆ -17.30999 ┆ -10.6     ┆ 0.316088 │\n",
       "│ 6         ┆           ┆ 9         ┆           ┆   ┆           ┆ 9         ┆           ┆          │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2020-01-0 ┆ -0.767834 ┆ 258.57000 ┆ 38.759998 ┆ … ┆ -19.4     ┆ -25.01    ┆ -14.21    ┆ -0.54880 │\n",
       "│ 7         ┆           ┆ 7         ┆           ┆   ┆           ┆           ┆           ┆ 1        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2020-01-0 ┆ -0.767834 ┆ 258.63000 ┆ 41.27     ┆ … ┆ -23.98    ┆ -30.26    ┆ -16.98    ┆ -0.65487 │\n",
       "│ 8         ┆           ┆ 5         ┆           ┆   ┆           ┆           ┆           ┆ 3        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 2020-01-0 ┆ -0.767834 ┆ 258.73001 ┆ 33.669998 ┆ … ┆ -20.01    ┆ -27.38999 ┆ -13.49    ┆ -0.55696 │\n",
       "│ 9         ┆           ┆ 1         ┆           ┆   ┆           ┆ 9         ┆           ┆ 1        │\n",
       "│ 00:00:00  ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6c692",
   "metadata": {},
   "source": [
    "## Splitting index entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce1d3ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_index_entreis_by_stage(\n",
    "    index_entries: list[dict],\n",
    ") -> dict[str, list[dict]]:\n",
    "    \"\"\"\n",
    "    Split index entries into train, val, and test sets based on their stage.\n",
    "\n",
    "    Args:\n",
    "        index_entries: List of index entries with stage information\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with keys 'train', 'val', and 'test' mapping to lists of index entries\n",
    "    \"\"\"\n",
    "    split_entries = {\"train\": [], \"val\": [], \"test\": []}\n",
    "\n",
    "    for entry in index_entries:\n",
    "        stage = entry[\"stage\"]\n",
    "        if stage in split_entries:\n",
    "            split_entries[stage].append(entry)\n",
    "\n",
    "    return split_entries\n",
    "\n",
    "\n",
    "# Example usage\n",
    "split_entries = split_index_entreis_by_stage(index_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d51c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fb0c8a0",
   "metadata": {},
   "source": [
    "## Dealing with static attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821bd9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read static attributes: 0.78807s\n"
     ]
    }
   ],
   "source": [
    "path_to_static = Path(\n",
    "    \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/testing_run_hydro_processor/processed_static_data/static_attributes.parquet\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_static_attributes_from_id(path, id):\n",
    "    static_df = pd.read_parquet(path_to_static)\n",
    "    static_df = static_df[static_df[\"gauge_id\"] == id]\n",
    "    return static_df\n",
    "\n",
    "\n",
    "statics = []\n",
    "start = time.time()\n",
    "for id in basins:\n",
    "    static = get_static_attributes_from_id(path_to_static, id)\n",
    "    statics.append(static)\n",
    "    del static\n",
    "end = time.time()\n",
    "print(f\"Time taken to read static attributes: {end - start:.5f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c2fe66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_attributes_from_id(static_df: pl.DataFrame, gauge_id: str, static_columns: list[str]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve static attributes for a specific gauge ID from a Polars DataFrame.\n",
    "\n",
    "    Args:\n",
    "        static_df: Polars DataFrame containing static attributes for all gauges.\n",
    "        gauge_id: The gauge ID to filter for.\n",
    "\n",
    "    Returns:\n",
    "        Polars DataFrame with static attributes for the specified gauge.\n",
    "    \"\"\"\n",
    "    static_df = pl.read_parquet(str(path_to_static), columns=static_columns)\n",
    "    filtered_df = static_df.filter(pl.col(\"gauge_id\") == gauge_id)\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def get_all_statics_for_basins(\n",
    "    path_to_static: Path, basin_ids: list[str], static_columns: list[str] = None\n",
    ") -> list[pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    Efficiently retrieve static attributes for a list of basin IDs.\n",
    "\n",
    "    Args:\n",
    "        path_to_static: Path to the static attributes parquet file.\n",
    "        basin_ids: List of basin (gauge) IDs.\n",
    "\n",
    "    Returns:\n",
    "        List of Polars DataFrames, one per basin ID.\n",
    "\n",
    "    Example:\n",
    "        >>> statics = get_all_statics_for_basins(Path(\"static_attributes.parquet\"), [\"CA-001\", \"CA-002\"])\n",
    "    \"\"\"\n",
    "    static_df = pl.read_parquet(path_to_static)\n",
    "    statics = []\n",
    "    start = time.time()\n",
    "    for gauge_id in basin_ids:\n",
    "        statics.append(get_static_attributes_from_id(static_df, gauge_id, static_columns))\n",
    "    end = time.time()\n",
    "    print(f\"Time taken to read static attributes: {end - start:.5f}s\")\n",
    "    return statics\n",
    "\n",
    "\n",
    "static_cols = [\n",
    "    \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "statics = get_static_attributes_from_id(path_to_static, \"CA_15013\", static_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c086d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>gauge_id</th><th>p_mean</th><th>area</th><th>ele_mt_sav</th><th>high_prec_dur</th><th>frac_snow</th><th>high_prec_freq</th><th>slp_dg_sav</th><th>cly_pc_sav</th><th>aridity_ERA5_LAND</th><th>aridity_FAO_PM</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;CA_15013&quot;</td><td>2.243786</td><td>254.78646</td><td>-0.099639</td><td>1.139241</td><td>0.378615</td><td>0.035202</td><td>174.240171</td><td>14.563168</td><td>0.971101</td><td>0.531601</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 11)\n",
       "┌──────────┬──────────┬───────────┬────────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ gauge_id ┆ p_mean   ┆ area      ┆ ele_mt_sav ┆ … ┆ slp_dg_sa ┆ cly_pc_sa ┆ aridity_E ┆ aridity_F │\n",
       "│ ---      ┆ ---      ┆ ---       ┆ ---        ┆   ┆ v         ┆ v         ┆ RA5_LAND  ┆ AO_PM     │\n",
       "│ str      ┆ f64      ┆ f64       ┆ f64        ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│          ┆          ┆           ┆            ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞══════════╪══════════╪═══════════╪════════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ CA_15013 ┆ 2.243786 ┆ 254.78646 ┆ -0.099639  ┆ … ┆ 174.24017 ┆ 14.563168 ┆ 0.971101  ┆ 0.531601  │\n",
       "│          ┆          ┆           ┆            ┆   ┆ 1         ┆           ┆           ┆           │\n",
       "└──────────┴──────────┴───────────┴────────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4145f9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
