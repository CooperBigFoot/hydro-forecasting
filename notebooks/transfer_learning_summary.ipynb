{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 14:22:38,995 - hydro_forecasting.experiment_utils.seed_manager - INFO - SeedManager initialized with master seed: 42\n",
      "2025-06-27 14:22:38,996 - lightning_fabric.utilities.seed - INFO - Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "from hydro_forecasting.experiment_utils.seed_manager import SeedManager  # noqa: E402\n",
    "\n",
    "seed_manager = SeedManager(42)\n",
    "seed_manager.set_global_seeds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydro_forecasting.experiment_utils.checkpoint_manager as checkpoint_manager\n",
    "from hydro_forecasting.data.caravanify_parquet import CaravanifyParquet, CaravanifyParquetConfig\n",
    "from hydro_forecasting.data.in_memory_datamodule import HydroInMemoryDataModule\n",
    "from hydro_forecasting.model_evaluation.evaluators import TSForecastEvaluator\n",
    "from hydro_forecasting.model_evaluation.hp_from_yaml import hp_from_yaml\n",
    "from hydro_forecasting.model_evaluation.visualization import (\n",
    "    plot_basin_performance_scatter,\n",
    "    plot_horizon_performance_bars,\n",
    "    plot_horizon_performance_boxplots,\n",
    "    plot_model_cdf_grid,\n",
    ")\n",
    "from hydro_forecasting.models.dummy import LitRepeatLastValues, RepeatLastValuesConfig\n",
    "from hydro_forecasting.models.ealstm import EALSTMConfig, LitEALSTM\n",
    "from hydro_forecasting.models.tft import LitTFT, TFTConfig\n",
    "from hydro_forecasting.models.tide import LitTiDE, TiDEConfig\n",
    "from hydro_forecasting.models.tsmixer import LitTSMixer, TSMixerConfig\n",
    "from hydro_forecasting.preprocessing import PipelineBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\"CA\"]\n",
    "\n",
    "COUNTRY = \"tajikistan\"\n",
    "\n",
    "MODEL_TYPES = [\n",
    "    \"tft\",\n",
    "    \"ealstm\",\n",
    "    \"tide\",\n",
    "    \"tsmixer\",\n",
    "]\n",
    "\n",
    "EXPERIMENT_TYPES = {\n",
    "    # \"pretrained\": f\"similar_catchments/similar-catchments_{COUNTRY.lower()}\",\n",
    "    # \"finetuned\": f\"finetune/finetune_from_similar_catchments_{COUNTRY.lower()}\",\n",
    "    \"benchmark\": f\"benchmark/benchmark_unified_with_RevIN_{COUNTRY.lower()}\",\n",
    "    # \"low_hii\": f\"low-medium-hii/low-medium-hii_{COUNTRY.lower()}\",\n",
    "}\n",
    "\n",
    "BASE_PATH = Path(\"/Users/cooper/Desktop/hydro-forecasting/experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - hidden_continuous_size (model-specific)\n",
      "  - quantiles (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "  - use_rev_in (standard)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - future_forcing_projection_size (model-specific)\n",
      "  - past_feature_projection_size (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "  - use_rev_in (standard)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - bias (model-specific)\n",
      "  - bidirectional (model-specific)\n",
      "  - bidirectional_fusion (model-specific)\n",
      "  - future_hidden_size (model-specific)\n",
      "  - future_layers (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "  - use_rev_in (standard)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "  - use_rev_in (standard)\n"
     ]
    }
   ],
   "source": [
    "ealstm_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/unified/{COUNTRY.lower()}/ealstm.yaml\"\n",
    "tft_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/unified/{COUNTRY.lower()}/tft.yaml\"\n",
    "tide_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/unified/{COUNTRY.lower()}/tide.yaml\"\n",
    "tsmixer_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/unified/{COUNTRY.lower()}/tsmixer.yaml\"\n",
    "\n",
    "\n",
    "tft_hp = hp_from_yaml(\"tft\", tft_yaml)\n",
    "tide_hp = hp_from_yaml(\"tide\", tide_yaml)\n",
    "ealstm_hp = hp_from_yaml(\"ealstm\", ealstm_yaml)\n",
    "tsmixer_hp = hp_from_yaml(\"tsmixer\", tsmixer_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFT_config = TFTConfig(**tft_hp)\n",
    "EALSTM_config = EALSTMConfig(**ealstm_hp)\n",
    "TiDE_config = TiDEConfig(**tide_hp)\n",
    "TSMixer_config = TSMixerConfig(**tsmixer_hp)\n",
    "\n",
    "dummy_config = RepeatLastValuesConfig(\n",
    "    input_len=ealstm_hp[\"input_len\"],\n",
    "    input_size=ealstm_hp[\"input_size\"],\n",
    "    output_len=ealstm_hp[\"output_len\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowInvalid",
     "evalue": "Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArrowInvalid\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     25\u001b[39m     static_data = caravan.get_static_attributes()\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(static_data[static_data[\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m] == country][\u001b[33m\"\u001b[39m\u001b[33mgauge_id\u001b[39m\u001b[33m\"\u001b[39m].unique())\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m country_ids = \u001b[43mload_basin_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOUNTRY\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mload_basin_ids\u001b[39m\u001b[34m(country)\u001b[39m\n\u001b[32m     22\u001b[39m caravan = CaravanifyParquet(configs)\n\u001b[32m     23\u001b[39m ca_basins = caravan.get_all_gauge_ids()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mcaravan\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_stations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mca_basins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m static_data = caravan.get_static_attributes()\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(static_data[static_data[\u001b[33m\"\u001b[39m\u001b[33mcountry\u001b[39m\u001b[33m\"\u001b[39m] == country][\u001b[33m\"\u001b[39m\u001b[33mgauge_id\u001b[39m\u001b[33m\"\u001b[39m].unique())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/data/caravanify_parquet.py:105\u001b[39m, in \u001b[36mCaravanifyParquet.load_stations\u001b[39m\u001b[34m(self, gauge_ids)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03mLoad station data for the specified gauge IDs.\u001b[39;00m\n\u001b[32m     94\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m \u001b[33;03m    FileNotFoundError: If required timeseries files are missing.\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_gauge_ids(gauge_ids)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_timeseries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgauge_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;28mself\u001b[39m._load_static_attributes(gauge_ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/data/caravanify_parquet.py:137\u001b[39m, in \u001b[36mCaravanifyParquet._load_timeseries\u001b[39m\u001b[34m(self, gauge_ids)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     dfs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dfs:\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m.time_series[df[\u001b[33m\"\u001b[39m\u001b[33mgauge_id\u001b[39m\u001b[33m\"\u001b[39m].iloc[\u001b[32m0\u001b[39m]] = df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.8-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/data/caravanify_parquet.py:130\u001b[39m, in \u001b[36mCaravanifyParquet._load_timeseries.<locals>.read_single\u001b[39m\u001b[34m(fp)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_single\u001b[39m(fp: Path) -> pd.DataFrame:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd.api.types.is_datetime64_any_dtype(df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m]):\n\u001b[32m    132\u001b[39m         df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1774\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1764\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1765\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1766\u001b[39m                memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m, partitioning=\u001b[33m\"\u001b[39m\u001b[33mhive\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1770\u001b[39m                thrift_container_size_limit=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1771\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1774\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1781\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1782\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1783\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1784\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1785\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1786\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1787\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1788\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1789\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1790\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1792\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1793\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1794\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pyarrow/parquet/core.py:1350\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[39m\n\u001b[32m   1346\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1347\u001b[39m     fragment = parquet_format.make_fragment(single_file, filesystem)\n\u001b[32m   1349\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset = ds.FileSystemDataset(\n\u001b[32m-> \u001b[39m\u001b[32m1350\u001b[39m         [fragment], schema=schema \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mfragment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mphysical_schema\u001b[49m,\n\u001b[32m   1351\u001b[39m         \u001b[38;5;28mformat\u001b[39m=parquet_format,\n\u001b[32m   1352\u001b[39m         filesystem=fragment.filesystem\n\u001b[32m   1353\u001b[39m     )\n\u001b[32m   1354\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1356\u001b[39m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pyarrow/_dataset.pyx:1473\u001b[39m, in \u001b[36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mArrowInvalid\u001b[39m: Could not open Parquet input source '<Buffer>': Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file."
     ]
    }
   ],
   "source": [
    "def load_basin_ids(country: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function to load basins for a given country in Central Asia\n",
    "    \"\"\"\n",
    "    # Make country lowercase and make the first letter uppercase\n",
    "    country = country.lower()\n",
    "    country = country.capitalize()\n",
    "\n",
    "    if country != \"Tajikistan\" and country != \"Kyrgyzstan\":\n",
    "        print(\"Country not supported\")\n",
    "        return []\n",
    "\n",
    "    configs = CaravanifyParquetConfig(\n",
    "        attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "        timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "        gauge_id_prefix=\"CA\",\n",
    "        use_hydroatlas_attributes=True,\n",
    "        use_caravan_attributes=True,\n",
    "        use_other_attributes=True,\n",
    "    )\n",
    "\n",
    "    caravan = CaravanifyParquet(configs)\n",
    "    ca_basins = caravan.get_all_gauge_ids()\n",
    "    caravan.load_stations(ca_basins)\n",
    "    static_data = caravan.get_static_attributes()\n",
    "\n",
    "    return list(static_data[static_data[\"country\"] == country][\"gauge_id\"].unique())\n",
    "\n",
    "\n",
    "country_ids = load_basin_ids(COUNTRY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(country_ids)} total CA basins in {COUNTRY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = PipelineBuilder()\n",
    "\n",
    "feature_section = (\n",
    "    builder.features().transforms([\"standard_scale\"]).strategy(\"unified\", fit_on_n_basins=200).columns(forcing_features)\n",
    ")\n",
    "\n",
    "target_section = (\n",
    "    builder.target()\n",
    "    .transforms([\"log_scale\", \"standard_scale\"])\n",
    "    .strategy(\"per_group\", group_by=\"gauge_id\")\n",
    "    .columns([target])\n",
    ")\n",
    "\n",
    "static_section = builder.static_features().transforms([\"standard_scale\"]).strategy(\"unified\").columns(static_features)\n",
    "\n",
    "preprocessing_config = builder.build()\n",
    "\n",
    "preprocessing_config[\"static_features\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/timeseries/csv/{region}\"\n",
    "    for region in REGIONS\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/attributes/{region}\" for region in REGIONS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_modules = {\n",
    "    name: HydroInMemoryDataModule(\n",
    "        region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "        region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "        path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/data_cache/first_eval\",\n",
    "        group_identifier=\"gauge_id\",\n",
    "        batch_size=2048,\n",
    "        input_length=hp[\"input_len\"],\n",
    "        output_length=hp[\"output_len\"],\n",
    "        forcing_features=forcing_features,\n",
    "        static_features=static_features,\n",
    "        target=target,\n",
    "        preprocessing_configs=preprocessing_config,\n",
    "        num_workers=4,\n",
    "        min_train_years=5,\n",
    "        train_prop=0.5,\n",
    "        val_prop=0.25,\n",
    "        test_prop=0.25,\n",
    "        max_imputation_gap_size=5,\n",
    "        list_of_gauge_ids_to_process=country_ids,\n",
    "        is_autoregressive=True,\n",
    "        chunk_size=100,\n",
    "        validation_chunk_size=100,\n",
    "        random_seed=42,\n",
    "    )\n",
    "    for name, hp in [\n",
    "        (\"tft_data_module\", tft_hp),\n",
    "        (\"tide_data_module\", tide_hp),\n",
    "        (\"tsmixer_data_module\", tsmixer_hp),\n",
    "        (\"ealstm_data_module\", ealstm_hp),\n",
    "    ]\n",
    "}\n",
    "\n",
    "tft_data_module = data_modules[\"tft_data_module\"]\n",
    "tide_data_module = data_modules[\"tide_data_module\"]\n",
    "tsmixer_data_module = data_modules[\"tsmixer_data_module\"]\n",
    "ealstm_data_module = data_modules[\"ealstm_data_module\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydro_forecasting.exceptions import ConfigurationError, FileOperationError\n",
    "\n",
    "\n",
    "def get_checkpoint_for_model(model_type: str, checkpoint_folder: Path, select_overall_best: bool = True) -> Path | None:\n",
    "    \"\"\"\n",
    "    Function to get the best checkpoint for a given model type.\n",
    "\n",
    "    Args:\n",
    "        model_type: Type of model to load checkpoint for\n",
    "        checkpoint_folder: Base directory containing checkpoints\n",
    "        select_overall_best: Whether to select the overall best model\n",
    "\n",
    "    Returns:\n",
    "        Path to checkpoint file, or None if not found or error occurred\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return checkpoint_manager.get_checkpoint_path_to_load(\n",
    "            base_checkpoint_load_dir=checkpoint_folder, model_type=model_type, select_overall_best=select_overall_best\n",
    "        )\n",
    "    except (ConfigurationError, FileOperationError) as e:\n",
    "        print(f\"Failed to load checkpoint for {model_type}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_all_checkpoints(\n",
    "    country: str, model_types: list[str] | None = None, experiment_types: dict[str, str] | None = None\n",
    ") -> dict[str, dict[str, Path | None]]:\n",
    "    \"\"\"\n",
    "    Load checkpoints for all model types across all experiment types.\n",
    "\n",
    "    Args:\n",
    "        country: Country code for the experiments\n",
    "        model_types: List of model types to load (defaults to common ones)\n",
    "        experiment_types: Dict mapping experiment names to directory patterns\n",
    "\n",
    "    Returns:\n",
    "        Dict of {experiment_type: {model_type: checkpoint_path}}\n",
    "    \"\"\"\n",
    "    # Default model types\n",
    "    if model_types is None:\n",
    "        model_types = [\"tft\", \"tide\", \"ealstm\", \"tsmixer\"]\n",
    "\n",
    "    base_path = BASE_PATH\n",
    "    checkpoints = {}\n",
    "\n",
    "    for exp_name, exp_path in experiment_types.items():\n",
    "        checkpoints[exp_name] = {}\n",
    "        checkpoint_dir = base_path / exp_path / \"checkpoints\"\n",
    "\n",
    "        for model_type in model_types:\n",
    "            checkpoint_path = get_checkpoint_for_model(\n",
    "                model_type=model_type, checkpoint_folder=checkpoint_dir, select_overall_best=True\n",
    "            )\n",
    "            checkpoints[exp_name][model_type] = checkpoint_path\n",
    "\n",
    "    return checkpoints\n",
    "\n",
    "\n",
    "def print_checkpoints(checkpoints: dict[str, dict[str, Path | None]]) -> None:\n",
    "    \"\"\"Print checkpoints in a readable format.\"\"\"\n",
    "    for exp_name, models in checkpoints.items():\n",
    "        valid_paths = [str(path) for path in models.values() if path is not None]\n",
    "        failed_models = [model for model, path in models.items() if path is None]\n",
    "\n",
    "        print(f\"{exp_name.capitalize()} checkpoints: {', '.join(valid_paths)}\")\n",
    "        if failed_models:\n",
    "            print(f\"  Failed to load: {', '.join(failed_models)}\")\n",
    "\n",
    "\n",
    "checkpoints = load_all_checkpoints(COUNTRY, MODEL_TYPES, EXPERIMENT_TYPES)\n",
    "print_checkpoints(checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tft_pretrained_checkpoint = checkpoints[\"pretrained\"][\"tft\"]\n",
    "# tide_pretrained_checkpoint = checkpoints[\"pretrained\"][\"tide\"]\n",
    "# ealstm_pretrained_checkpoint = checkpoints[\"pretrained\"][\"ealstm\"]\n",
    "# tsmixer_pretrained_checkpoint = checkpoints[\"pretrained\"][\"tsmixer\"]\n",
    "\n",
    "# tft_finetuned_checkpoint = checkpoints[\"finetuned\"][\"tft\"]\n",
    "# tide_finetuned_checkpoint = checkpoints[\"finetuned\"][\"tide\"]\n",
    "# ealstm_finetuned_checkpoint = checkpoints[\"finetuned\"][\"ealstm\"]\n",
    "# tsmixer_finetuned_checkpoint = checkpoints[\"finetuned\"][\"tsmixer\"]\n",
    "\n",
    "tft_benchmark_checkpoint = checkpoints[\"benchmark\"][\"tft\"]\n",
    "tide_benchmark_checkpoint = checkpoints[\"benchmark\"][\"tide\"]\n",
    "ealstm_benchmark_checkpoint = checkpoints[\"benchmark\"][\"ealstm\"]\n",
    "tsmixer_benchmark_checkpoint = checkpoints[\"benchmark\"][\"tsmixer\"]\n",
    "\n",
    "# tft_low_hii_checkpoint = checkpoints[\"low_hii\"][\"tft\"]\n",
    "# tide_low_hii_checkpoint = checkpoints[\"low_hii\"][\"tide\"]\n",
    "# ealstm_low_hii_checkpoint = checkpoints[\"low_hii\"][\"ealstm\"]\n",
    "# tsmixer_low_hii_checkpoint = checkpoints[\"low_hii\"][\"tsmixer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = LitRepeatLastValues(config=dummy_config)\n",
    "ealstm_benchmark_model = LitEALSTM.load_from_checkpoint(ealstm_benchmark_checkpoint, config=EALSTM_config)\n",
    "tide_benchmark_model = LitTiDE.load_from_checkpoint(tide_benchmark_checkpoint, config=TiDE_config)\n",
    "tsmixer_benchmark_model = LitTSMixer.load_from_checkpoint(tsmixer_benchmark_checkpoint, config=TSMixer_config)\n",
    "tft_benchmark_model = LitTFT.load_from_checkpoint(tft_benchmark_checkpoint, config=TFT_config)\n",
    "\n",
    "# ealstm_pretrained_model = LitEALSTM.load_from_checkpoint(ealstm_pretrained_checkpoint, config=EALSTM_config)\n",
    "# tide_pretrained_model = LitTiDE.load_from_checkpoint(tide_pretrained_checkpoint, config=TiDE_config)\n",
    "# tsmixer_pretrained_model = LitTSMixer.load_from_checkpoint(tsmixer_pretrained_checkpoint, config=TSMixer_config)\n",
    "# tft_pretrained_model = LitTFT.load_from_checkpoint(tft_pretrained_checkpoint, config=TFT_config)\n",
    "\n",
    "# ealstm_finetuned_model = LitEALSTM.load_from_checkpoint(ealstm_finetuned_checkpoint, config=EALSTM_config)\n",
    "# tide_finetuned_model = LitTiDE.load_from_checkpoint(tide_finetuned_checkpoint, config=TiDE_config)\n",
    "# tsmixer_finetuned_model = LitTSMixer.load_from_checkpoint(tsmixer_finetuned_checkpoint, config=TSMixer_config)\n",
    "# tft_finetuned_model = LitTFT.load_from_checkpoint(tft_finetuned_checkpoint, config=TFT_config)\n",
    "\n",
    "# tft_low_hii_model = LitTFT.load_from_checkpoint(tft_low_hii_checkpoint, config=TFT_config)\n",
    "# tide_low_hii_model = LitTiDE.load_from_checkpoint(tide_low_hii_checkpoint, config=TiDE_config)\n",
    "# ealstm_low_hii_model = LitEALSTM.load_from_checkpoint(ealstm_low_hii_checkpoint, config=EALSTM_config)\n",
    "# tsmixer_low_hii_model = LitTSMixer.load_from_checkpoint(tsmixer_low_hii_checkpoint, config=TSMixer_config)\n",
    "\n",
    "\n",
    "# Create a dictionary mapping model names to (model, datamodule) tuples\n",
    "models_and_datamodules = {\n",
    "    \"ealstm_benchmark\": (ealstm_benchmark_model, ealstm_data_module),\n",
    "    # \"ealstm_pretrained\": (ealstm_pretrained_model, ealstm_data_module),\n",
    "    # \"ealstm_finetuned\": (ealstm_finetuned_model, ealstm_data_module),\n",
    "    \"tide_benchmark\": (tide_benchmark_model, tide_data_module),\n",
    "    # \"tide_pretrained\": (tide_pretrained_model, tide_data_module),\n",
    "    # \"tide_finetuned\": (tide_finetuned_model, tide_data_module),\n",
    "    \"tsmixer_benchmark\": (tsmixer_benchmark_model, tsmixer_data_module),\n",
    "    # \"tsmixer_pretrained\": (tsmixer_pretrained_model, tsmixer_data_module),\n",
    "    # \"tsmixer_finetuned\": (tsmixer_finetuned_model, tsmixer_data_module),\n",
    "    \"tft_benchmark\": (tft_benchmark_model, tft_data_module),\n",
    "    # \"tft_pretrained\": (tft_pretrained_model, tft_data_module),\n",
    "    # \"tft_finetuned\": (tft_finetuned_model, tft_data_module),\n",
    "    # \"tft_low_hii\": (tft_low_hii_model, tft_data_module),\n",
    "    # \"tide_low_hii\": (tide_low_hii_model, tide_data_module),\n",
    "    # \"ealstm_low_hii\": (ealstm_low_hii_model, ealstm_data_module),\n",
    "    # \"tsmixer_low_hii\": (tsmixer_low_hii_model, tsmixer_data_module),\n",
    "    \"dummy\": (dummy_model, ealstm_data_module),\n",
    "}\n",
    "\n",
    "\n",
    "evaluator = TSForecastEvaluator(\n",
    "    horizons=list(range(1, 11)),\n",
    "    models_and_datamodules=models_and_datamodules,\n",
    "    trainer_kwargs={\"accelerator\": \"mps\", \"devices\": 1, \"deterministic\": True, \"benchmark\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = evaluator.test_models(\n",
    "    start_of_season=4,\n",
    "    end_of_season=10,\n",
    "    cache_path=f\"/Users/cooper/Desktop/hydro-forecasting/data_cache/model_evaluation_cache_{COUNTRY.lower()}\",\n",
    "    force_refresh=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(context=\"paper\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_horizon_performance_bars(\n",
    "    results,\n",
    "    horizon=10,\n",
    "    metric=\"nse\",\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    variants=[\"benchmark\"],\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    figsize=(8, 5),\n",
    "    with_whiskers=False,\n",
    "    positive_is_better=True,\n",
    "    dummy_model=\"dummy\",\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_horizon_performance_boxplots(\n",
    "    results,\n",
    "    horizons=[1, 5, 10],\n",
    "    metric=\"nse\",\n",
    "    architectures=[\"ealstm\", \"tide\", \"tsmixer\", \"tft\"],\n",
    "    variants=[\"benchmark\"],\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    figsize=(12, 6),\n",
    "    show_median_labels=True,\n",
    "    dummy_model=\"dummy\",\n",
    ")\n",
    "\n",
    "ax.set_ylim(0.2, 1)\n",
    "ax.set_title(\"\")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_basin_performance_scatter(\n",
    "    results,\n",
    "    benchmark_pattern=\"pretrained\",\n",
    "    challenger_pattern=\"finetuned\",\n",
    "    horizon=10,\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    metric=\"nse\",\n",
    "    figsize=(10, 6),\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    debug=True,\n",
    ")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_model_cdf_grid(\n",
    "    results,\n",
    "    horizons=[1, 5, 10],\n",
    "    metric=\"nse\",\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    variants=[\"benchmark\", \"low_hii\"],\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    figsize=(12, 8),\n",
    ")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plot_rolling_forecast(\n",
    "#     results,\n",
    "#     model_name=\"tsmixer_finetuned\",\n",
    "#     gauge_id=\"CA_15016\",\n",
    "#     horizon=1,\n",
    "#     figsize=(10, 5),\n",
    "#     color_scheme={\"observed\": \"#2E4057\", \"predicted\": \"#FF6B35\"},\n",
    "# )\n",
    "\n",
    "# sns.despine()\n",
    "# ax.set_xlabel(\"\")\n",
    "# ax.set_ylabel(\"Streamflow (mm/day)\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_ca = CaravanifyParquetConfig(\n",
    "#     attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "#     timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "#     shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "#     gauge_id_prefix=\"CA\",\n",
    "#     use_hydroatlas_attributes=True,\n",
    "#     use_caravan_attributes=True,\n",
    "#     use_other_attributes=True,\n",
    "# )\n",
    "\n",
    "# caravan_ca = CaravanifyParquet(config_ca)\n",
    "\n",
    "# caravan_ca._load_static_attributes(country_ids)\n",
    "# statics = caravan_ca.get_static_attributes()\n",
    "\n",
    "\n",
    "# fig, axes = plot_performance_vs_static_attributes(\n",
    "#     results,\n",
    "#     static_df=statics,\n",
    "#     static_attributes=[\"p_mean\", \"ele_mt_sav\", \"frac_snow\"],\n",
    "#     model_names=[\"tide_low_hii\"],\n",
    "#     horizons=[5],\n",
    "#     metric=\"nse\",\n",
    "#     figsize=(11, 4),\n",
    "#     attribute_labels={\n",
    "#         \"p_mean\": \"Mean Precipitation (mm/d)\",\n",
    "#         \"ele_mt_sav\": \"Mean Elevation (m.a.s.l.)\",\n",
    "#         \"frac_snow\": \"Precipitation Falling as Snow (%)\",\n",
    "#     },\n",
    "# )\n",
    "\n",
    "# sns.despine()\n",
    "# # set y lim to (0, 1)\n",
    "# for ax in axes.flatten():\n",
    "#     ax.set_ylim(0.0, 1)\n",
    "#     ax.set_title(\"\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydro-forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
