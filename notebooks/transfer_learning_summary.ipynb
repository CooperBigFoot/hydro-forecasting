{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/cooper/Desktop/hydro-forecasting/src to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from pytorch_lightning import seed_everything\n",
    "from returns.result import Failure, Result, Success\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "import hydro_forecasting.experiment_utils.checkpoint_manager as checkpoint_manager\n",
    "from hydro_forecasting.data.caravanify_parquet import CaravanifyParquet, CaravanifyParquetConfig\n",
    "from hydro_forecasting.data.in_memory_datamodule import HydroInMemoryDataModule\n",
    "from hydro_forecasting.model_evaluation.evaluators import TSForecastEvaluator\n",
    "from hydro_forecasting.model_evaluation.hp_from_yaml import hp_from_yaml\n",
    "from hydro_forecasting.model_evaluation.visualization import (\n",
    "    plot_basin_performance_scatter,\n",
    "    plot_horizon_performance_bars,\n",
    "    plot_model_cdf_grid,\n",
    ")\n",
    "from hydro_forecasting.models.dummy import LitRepeatLastValues, RepeatLastValuesConfig\n",
    "from hydro_forecasting.models.ealstm import EALSTMConfig, LitEALSTM\n",
    "from hydro_forecasting.models.tft import LitTFT, TFTConfig\n",
    "from hydro_forecasting.models.tide import LitTiDE, TiDEConfig\n",
    "from hydro_forecasting.models.tsmixer import LitTSMixer, TSMixerConfig\n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\"CA\"]\n",
    "\n",
    "COUNTRY = \"tajikistan\"\n",
    "\n",
    "MODEL_TYPES = [\n",
    "    \"tft\",\n",
    "    \"ealstm\",\n",
    "    \"tide\",\n",
    "    \"tsmixer\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - hidden_continuous_size (model-specific)\n",
      "  - quantiles (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - future_forcing_projection_size (model-specific)\n",
      "  - past_feature_projection_size (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - bias (model-specific)\n",
      "  - bidirectional (model-specific)\n",
      "  - bidirectional_fusion (model-specific)\n",
      "  - future_hidden_size (model-specific)\n",
      "  - future_layers (model-specific)\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n",
      "The following parameters were not found in the YAML file and will use defaults:\n",
      "  - scheduler_factor (model-specific)\n",
      "  - scheduler_patience (model-specific)\n"
     ]
    }
   ],
   "source": [
    "ealstm_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY.lower()}/ealstm.yaml\"\n",
    "tft_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY.lower()}/tft.yaml\"\n",
    "tide_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY.lower()}/tide.yaml\"\n",
    "tsmixer_yaml = f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY.lower()}/tsmixer.yaml\"\n",
    "\n",
    "\n",
    "tft_hp = hp_from_yaml(\"tft\", tft_yaml)\n",
    "tide_hp = hp_from_yaml(\"tide\", tide_yaml)\n",
    "ealstm_hp = hp_from_yaml(\"ealstm\", ealstm_yaml)\n",
    "tsmixer_hp = hp_from_yaml(\"tsmixer\", tsmixer_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TFT_config = TFTConfig(**tft_hp)\n",
    "EALSTM_config = EALSTMConfig(**ealstm_hp)\n",
    "TiDE_config = TiDEConfig(**tide_hp)\n",
    "TSMixer_config = TSMixerConfig(**tsmixer_hp)\n",
    "\n",
    "dummy_config = RepeatLastValuesConfig(\n",
    "    input_len=ealstm_hp[\"input_len\"],\n",
    "    input_size=ealstm_hp[\"input_size\"],\n",
    "    output_len=ealstm_hp[\"output_len\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_basin_ids(country: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function to load basins for a given country in Central Asia\n",
    "    \"\"\"\n",
    "    # Make country lowercase and make the first letter uppercase\n",
    "    country = country.lower()\n",
    "    country = country.capitalize()\n",
    "\n",
    "    if country != \"Tajikistan\" and country != \"Kyrgyzstan\":\n",
    "        print(\"Country not supported\")\n",
    "        return []\n",
    "\n",
    "    configs = CaravanifyParquetConfig(\n",
    "        attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "        timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "        gauge_id_prefix=\"CA\",\n",
    "        use_hydroatlas_attributes=True,\n",
    "        use_caravan_attributes=True,\n",
    "        use_other_attributes=True,\n",
    "    )\n",
    "\n",
    "    caravan = CaravanifyParquet(configs)\n",
    "    ca_basins = caravan.get_all_gauge_ids()\n",
    "    caravan.load_stations(ca_basins)\n",
    "    static_data = caravan.get_static_attributes()\n",
    "\n",
    "    return list(static_data[static_data[\"country\"] == country][\"gauge_id\"].unique())\n",
    "\n",
    "\n",
    "country_ids = load_basin_ids(COUNTRY)\n",
    "country_ids = [id for id in country_ids if id != \"CA_15030\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 total CA basins in tajikistan\n"
     ]
    }
   ],
   "source": [
    "print(f\"Found {len(country_ids)} total CA basins in {COUNTRY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/timeseries/csv/{region}\"\n",
    "    for region in REGIONS\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/attributes/{region}\" for region in REGIONS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tft_data_module = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/first_eval\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=tft_hp[\"input_len\"],\n",
    "    output_length=tft_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=country_ids,\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=100,\n",
    "    validation_chunk_size=100,\n",
    ")\n",
    "\n",
    "tide_data_module = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/first_eval\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=tide_hp[\"input_len\"],\n",
    "    output_length=tide_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=country_ids,\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=100,\n",
    "    validation_chunk_size=100,\n",
    ")\n",
    "\n",
    "tsmixer_data_module = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/first_eval\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=tsmixer_hp[\"input_len\"],\n",
    "    output_length=tsmixer_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=country_ids,\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=100,\n",
    "    validation_chunk_size=100,\n",
    ")\n",
    "\n",
    "ealstm_data_module = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/first_eval\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=ealstm_hp[\"input_len\"],\n",
    "    output_length=ealstm_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=country_ids,\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=100,\n",
    "    validation_chunk_size=100,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoint_for_model(model_type: str, checkpoint_folder: Path, select_overall_best: bool = True):\n",
    "    \"\"\"\n",
    "    Function to get the best checkpoint for a given model type\n",
    "    \"\"\"\n",
    "    result = checkpoint_manager.get_checkpoint_path_to_load(\n",
    "        base_checkpoint_load_dir=checkpoint_folder, model_type=model_type, select_overall_best=select_overall_best\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "def unwrap(checkpoint: Result) -> Path:\n",
    "    \"\"\"\n",
    "    Function to unwrap the checkpoint result\n",
    "    \"\"\"\n",
    "    if isinstance(checkpoint, Failure):\n",
    "        print(f\"Failed to load checkpoint: {checkpoint.failure()}\")\n",
    "        return None\n",
    "    elif isinstance(checkpoint, Success):\n",
    "        return checkpoint.unwrap()\n",
    "\n",
    "\n",
    "pretrained_checkpoint_dir = Path(\n",
    "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_{COUNTRY.lower()}/checkpoints\"\n",
    ")\n",
    "\n",
    "finetuned_checkpoint_dir = Path(\n",
    "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/finetune/finetune_from_low-medium-hii_{COUNTRY.lower()}/checkpoints\"\n",
    ")\n",
    "\n",
    "benchmark_checkpoint_dir = Path(\n",
    "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/benchmark/benchmark_{COUNTRY.lower()}/checkpoints\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained checkpoints: /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tide/run_0/attempt_0/tide-run0-attempt_0-epoch=75-val_loss=0.0446.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/ealstm/run_0/attempt_0/ealstm-run0-attempt_0-epoch=43-val_loss=0.0924.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tsmixer/run_0/attempt_0/tsmixer-run0-attempt_0-epoch=41-val_loss=0.0934.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tft/run_0/attempt_0/tft-run0-attempt_0-epoch=154-val_loss=0.0380.ckpt\n",
      "Finetuned checkpoints: /Users/cooper/Desktop/hydro-forecasting/experiments/finetune/finetune_from_low-medium-hii_tajikistan/checkpoints/tide/run_3/attempt_0/tide-run3-attempt_0-epoch=13-val_loss=0.0298.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/finetune/finetune_from_low-medium-hii_tajikistan/checkpoints/ealstm/run_4/attempt_0/ealstm-run4-attempt_0-epoch=01-val_loss=0.0258.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/finetune/finetune_from_low-medium-hii_tajikistan/checkpoints/tsmixer/run_0/attempt_0/tsmixer-run0-attempt_0-epoch=11-val_loss=0.0270.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/finetune/finetune_from_low-medium-hii_tajikistan/checkpoints/tft/run_0/attempt_0/tft-run0-attempt_0-epoch=49-val_loss=0.0273.ckpt\n",
      "Benchmark checkpoints: /Users/cooper/Desktop/hydro-forecasting/experiments/benchmark/benchmark_tajikistan/checkpoints/tide/run_3/attempt_0/tide-run3-attempt_0-epoch=67-val_loss=0.0361.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/benchmark/benchmark_tajikistan/checkpoints/ealstm/run_2/attempt_0/ealstm-run2-attempt_0-epoch=12-val_loss=0.0345.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/benchmark/benchmark_tajikistan/checkpoints/tsmixer/run_0/attempt_0/tsmixer-run0-attempt_0-epoch=28-val_loss=0.0395.ckpt, /Users/cooper/Desktop/hydro-forecasting/experiments/benchmark/benchmark_tajikistan/checkpoints/tft/run_2/attempt_0/tft-run2-attempt_0-epoch=197-val_loss=0.0341.ckpt\n"
     ]
    }
   ],
   "source": [
    "tft_pretrained_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tft\",\n",
    "        checkpoint_folder=pretrained_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tide_pretrained_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tide\",\n",
    "        checkpoint_folder=pretrained_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "ealstm_pretrained_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"ealstm\",\n",
    "        checkpoint_folder=pretrained_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "tsmixer_pretrained_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tsmixer\",\n",
    "        checkpoint_folder=pretrained_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tft_finetuned_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tft\",\n",
    "        checkpoint_folder=finetuned_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tide_finetuned_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tide\",\n",
    "        checkpoint_folder=finetuned_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "ealstm_finetuned_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"ealstm\",\n",
    "        checkpoint_folder=finetuned_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "tsmixer_finetuned_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tsmixer\",\n",
    "        checkpoint_folder=finetuned_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tft_benchmark_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tft\",\n",
    "        checkpoint_folder=benchmark_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tide_benchmark_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tide\",\n",
    "        checkpoint_folder=benchmark_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "ealstm_benchmark_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"ealstm\",\n",
    "        checkpoint_folder=benchmark_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "tsmixer_benchmark_checkpoint = unwrap(\n",
    "    get_checkpoint_for_model(\n",
    "        model_type=\"tsmixer\",\n",
    "        checkpoint_folder=benchmark_checkpoint_dir,\n",
    "        select_overall_best=True,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Pretrained checkpoints: {tide_pretrained_checkpoint}, {ealstm_pretrained_checkpoint}, {tsmixer_pretrained_checkpoint}, {tft_pretrained_checkpoint}\"\n",
    ")\n",
    "print(\n",
    "    f\"Finetuned checkpoints: {tide_finetuned_checkpoint}, {ealstm_finetuned_checkpoint}, {tsmixer_finetuned_checkpoint}, {tft_finetuned_checkpoint}\"\n",
    ")\n",
    "print(\n",
    "    f\"Benchmark checkpoints: {tide_benchmark_checkpoint}, {ealstm_benchmark_checkpoint}, {tsmixer_benchmark_checkpoint}, {tft_benchmark_checkpoint}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = LitRepeatLastValues(config=dummy_config)\n",
    "ealstm_benchmark_model = LitEALSTM.load_from_checkpoint(ealstm_benchmark_checkpoint, config=EALSTM_config)\n",
    "tide_benchmark_model = LitTiDE.load_from_checkpoint(tide_benchmark_checkpoint, config=TiDE_config)\n",
    "tsmixer_benchmark_model = LitTSMixer.load_from_checkpoint(tsmixer_benchmark_checkpoint, config=TSMixer_config)\n",
    "tft_benchmark_model = LitTFT.load_from_checkpoint(tft_benchmark_checkpoint, config=TFT_config)\n",
    "\n",
    "ealstm_pretrained_model = LitEALSTM.load_from_checkpoint(ealstm_pretrained_checkpoint, config=EALSTM_config)\n",
    "tide_pretrained_model = LitTiDE.load_from_checkpoint(tide_pretrained_checkpoint, config=TiDE_config)\n",
    "tsmixer_pretrained_model = LitTSMixer.load_from_checkpoint(tsmixer_pretrained_checkpoint, config=TSMixer_config)\n",
    "tft_pretrained_model = LitTFT.load_from_checkpoint(tft_pretrained_checkpoint, config=TFT_config)\n",
    "\n",
    "ealstm_finetuned_model = LitEALSTM.load_from_checkpoint(ealstm_finetuned_checkpoint, config=EALSTM_config)\n",
    "tide_finetuned_model = LitTiDE.load_from_checkpoint(tide_finetuned_checkpoint, config=TiDE_config)\n",
    "tsmixer_finetuned_model = LitTSMixer.load_from_checkpoint(tsmixer_finetuned_checkpoint, config=TSMixer_config)\n",
    "tft_finetuned_model = LitTFT.load_from_checkpoint(tft_finetuned_checkpoint, config=TFT_config)\n",
    "\n",
    "\n",
    "# Create a dictionary mapping model names to (model, datamodule) tuples\n",
    "models_and_datamodules = {\n",
    "    \"ealstm_benchmark\": (ealstm_benchmark_model, ealstm_data_module),\n",
    "    # \"ealstm_pretrained\": (ealstm_pretrained_model, ealstm_data_module),\n",
    "    # \"ealstm_finetuned\": (ealstm_finetuned_model, ealstm_data_module),\n",
    "    # \"tide_benchmark\": (tide_benchmark_model, tide_data_module),\n",
    "    # \"tide_pretrained\": (tide_pretrained_model, tide_data_module),\n",
    "    # \"tide_finetuned\": (tide_finetuned_model, tide_data_module),\n",
    "    # \"tsmixer_benchmark\": (tsmixer_benchmark_model, tsmixer_data_module),\n",
    "    # \"tsmixer_pretrained\": (tsmixer_pretrained_model, tsmixer_data_module),\n",
    "    # \"tsmixer_finetuned\": (tsmixer_finetuned_model, tsmixer_data_module),\n",
    "    # \"tft_benchmark\": (tft_benchmark_model, tft_data_module),\n",
    "    # \"tft_pretrained\": (tft_pretrained_model, tft_data_module),\n",
    "    # \"tft_finetuned\": (tft_finetuned_model, tft_data_module),\n",
    "    \"dummy\": (dummy_model, ealstm_data_module),\n",
    "}\n",
    "\n",
    "\n",
    "evaluator = TSForecastEvaluator(\n",
    "    horizons=list(range(1, 11)),\n",
    "    models_and_datamodules=models_and_datamodules,\n",
    "    trainer_kwargs={\"accelerator\": \"cpu\", \"devices\": 1, \"deterministic\": True, \"benchmark\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 08:55:17,718 - hydro_forecasting.model_evaluation.evaluators - INFO - Testing model: ealstm_benchmark\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "2025-06-11 08:55:17,768 - hydro_forecasting.data.in_memory_datamodule - INFO - Starting data preparation...\n",
      "2025-06-11 08:55:17,770 - hydro_forecasting.data.in_memory_datamodule - INFO - Generated Run UUID for current config: e6a9396c-5a5e-5c0d-9aa7-2ee9cd109f78\n",
      "2025-06-11 08:55:17,772 - hydro_forecasting.data.in_memory_datamodule - INFO - Checking for existing processed data at: /Users/cooper/Desktop/hydro-forecasting/tests/first_eval/e6a9396c-5a5e-5c0d-9aa7-2ee9cd109f78\n",
      "2025-06-11 08:55:17,786 - hydro_forecasting.data.in_memory_datamodule - INFO - Successfully validated and prepared to reuse data from /Users/cooper/Desktop/hydro-forecasting/tests/first_eval/e6a9396c-5a5e-5c0d-9aa7-2ee9cd109f78\n",
      "2025-06-11 08:55:17,786 - hydro_forecasting.data.in_memory_datamodule - INFO - Reusing existing processed data from run_uuid: e6a9396c-5a5e-5c0d-9aa7-2ee9cd109f78\n",
      "2025-06-11 08:55:17,787 - hydro_forecasting.data.in_memory_datamodule - INFO - Loaded 3 pipelines and data for 16 basins from reused run.\n",
      "2025-06-11 08:55:17,794 - hydro_forecasting.data.in_memory_datamodule - INFO - Found 16 basins for synchronized train/val chunking and validation pool selection.\n",
      "2025-06-11 08:55:17,794 - hydro_forecasting.data.in_memory_datamodule - INFO - Found 16 basins for test split.\n",
      "2025-06-11 08:55:17,794 - hydro_forecasting.data.in_memory_datamodule - INFO - Data preparation finished.\n",
      "2025-06-11 08:55:17,858 - hydro_forecasting.data.in_memory_datamodule - INFO - Loading static data cache and converting to Tensors...\n",
      "2025-06-11 08:55:17,861 - hydro_forecasting.data.in_memory_datamodule - INFO - Loaded and tensorized static data for 16 basins.\n",
      "2025-06-11 08:55:17,916 - hydro_forecasting.data.in_memory_datamodule - INFO - Loading test data for 16 basins...\n",
      "2025-06-11 08:55:17,938 - hydro_forecasting.data.in_memory_datamodule - INFO - Stage 'test' chunk data loaded for 16 basins. Shape: (26619, 12). Est. Mem: 1.22 MB\n",
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3970599dc79a4d0d863831f97d99afb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.03726644441485405\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 08:55:30,718 - hydro_forecasting.model_evaluation.evaluators - INFO - Successfully tested model: ealstm_benchmark\n",
      "2025-06-11 08:55:30,718 - hydro_forecasting.model_evaluation.evaluators - INFO - Testing model: dummy\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "2025-06-11 08:55:30,732 - hydro_forecasting.data.in_memory_datamodule - INFO - Data preparation has already run.\n",
      "2025-06-11 08:55:30,739 - hydro_forecasting.data.in_memory_datamodule - INFO - Loading test data for 16 basins...\n",
      "2025-06-11 08:55:30,758 - hydro_forecasting.data.in_memory_datamodule - INFO - Stage 'test' chunk data loaded for 16 basins. Shape: (26619, 12). Est. Mem: 1.22 MB\n",
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea626e2e9b94ba4b4dac1efc9687b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss           0.7074034810066223\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 08:55:37,622 - hydro_forecasting.model_evaluation.evaluators - INFO - Successfully tested model: dummy\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "results = evaluator.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['predictions_df', 'metrics_by_gauge'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"ealstm_benchmark\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     67\u001b[39m         seasonal_results[key] = seasonal_model_results\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m seasonal_results\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m seasonal_results = \u001b[43mprocess_seasonal_results_polars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodels_and_datamodules\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mprocess_seasonal_results_polars\u001b[39m\u001b[34m(results, evaluator, model_keys)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Filter for growing season using the Polars version\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m seasonal_model_results = \u001b[43mfilter_growing_season_polars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m seasonal_model_results[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m] = evaluator._calculate_overall_metrics(seasonal_model_results[\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     64\u001b[39m seasonal_model_results[\u001b[33m\"\u001b[39m\u001b[33mbasin_metrics\u001b[39m\u001b[33m\"\u001b[39m] = evaluator._calculate_basin_metrics(seasonal_model_results[\u001b[33m\"\u001b[39m\u001b[33mdf\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mfilter_growing_season_polars\u001b[39m\u001b[34m(eval_results)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03mFilter evaluation results to include only data from the growing season (April to October), using Polars.\u001b[39;00m\n\u001b[32m      7\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m    dictionary with filtered Polars DataFrame and original metrics.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m filtered_results = eval_results.copy()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m df = \u001b[43meval_results\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.clone()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataFrame must contain a \u001b[39m\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m\u001b[33m column.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'df'"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def filter_growing_season(eval_results: dict[str, Any]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Filter evaluation results to include only data from the growing season (April to October).\n",
    "\n",
    "    Args:\n",
    "        eval_results: Dictionary containing evaluation results with a 'predictions_df' key,\n",
    "                      where 'predictions_df' is a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with filtered DataFrame and recalculated metrics.\n",
    "    \"\"\"\n",
    "    filtered_results = eval_results.copy()\n",
    "\n",
    "    predictions_df = eval_results[\"predictions_df\"].copy()\n",
    "\n",
    "    if \"date\" not in predictions_df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'date' column.\")\n",
    "\n",
    "    # Filter for growing season (April to September, inclusive)\n",
    "    growing_season_mask = (predictions_df[\"date\"].dt.month >= 4) & (predictions_df[\"date\"].dt.month <= 9)\n",
    "    growing_season_df = predictions_df[growing_season_mask]\n",
    "\n",
    "    filtered_results[\"predictions_df\"] = growing_season_df\n",
    "\n",
    "    return filtered_results\n",
    "\n",
    "\n",
    "def recalculate_metrics_from_predictions_df(\n",
    "    predictions_df: pd.DataFrame, horizons: list[int]\n",
    ") -> dict[str, dict[str, dict[str, float]]]:\n",
    "    \"\"\"\n",
    "    Recalculate metrics by gauge and horizon from predictions DataFrame.\n",
    "\n",
    "    Args:\n",
    "        predictions_df: DataFrame with columns [\"horizon\", \"observed\", \"predicted\", \"date\", \"gauge_id\"]\n",
    "        horizons: List of forecast horizons to evaluate\n",
    "\n",
    "    Returns:\n",
    "        Nested dictionary with structure: {gauge_id: {horizon_X: {metric: value}}}\n",
    "    \"\"\"\n",
    "    from .metrics import (\n",
    "        calculate_atpe,\n",
    "        calculate_kge,\n",
    "        calculate_mae,\n",
    "        calculate_mse,\n",
    "        calculate_nse,\n",
    "        calculate_pbias,\n",
    "        calculate_pearson_r,\n",
    "        calculate_rmse,\n",
    "    )\n",
    "\n",
    "    # Define all metrics functions\n",
    "    metric_functions = {\n",
    "        \"mse\": calculate_mse,\n",
    "        \"mae\": calculate_mae,\n",
    "        \"rmse\": calculate_rmse,\n",
    "        \"nse\": calculate_nse,\n",
    "        \"pearson_r\": calculate_pearson_r,\n",
    "        \"kge\": calculate_kge,\n",
    "        \"pbias\": calculate_pbias,\n",
    "        \"atpe\": calculate_atpe,\n",
    "    }\n",
    "\n",
    "    metrics_by_gauge = {}\n",
    "\n",
    "    for gauge_id in predictions_df[\"gauge_id\"].unique():\n",
    "        gauge_df = predictions_df[predictions_df[\"gauge_id\"] == gauge_id]\n",
    "        metrics_by_gauge[gauge_id] = {}\n",
    "\n",
    "        for horizon in horizons:\n",
    "            horizon_df = gauge_df[gauge_df[\"horizon\"] == horizon]\n",
    "\n",
    "            if horizon_df.empty:\n",
    "                continue\n",
    "\n",
    "            observed = horizon_df[\"observed\"].values\n",
    "            predicted = horizon_df[\"predicted\"].values\n",
    "\n",
    "            # Calculate all metrics\n",
    "            horizon_metrics = {}\n",
    "            for metric_name, metric_func in metric_functions.items():\n",
    "                try:\n",
    "                    metric_value = metric_func(predicted, observed)\n",
    "                    horizon_metrics[metric_name] = metric_value\n",
    "                except Exception as e:\n",
    "                    # Use np.nan for failed calculations\n",
    "                    import numpy as np\n",
    "\n",
    "                    horizon_metrics[metric_name] = np.nan\n",
    "\n",
    "            metrics_by_gauge[gauge_id][f\"horizon_{horizon}\"] = horizon_metrics\n",
    "\n",
    "    return metrics_by_gauge\n",
    "\n",
    "\n",
    "def process_seasonal_results(\n",
    "    results: dict[str, Any],\n",
    "    horizons: list[int],\n",
    "    model_keys: list[str] | None = None,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process specified model results to get seasonal metrics using pandas DataFrames.\n",
    "\n",
    "    Args:\n",
    "        results: Dictionary containing results for all models.\n",
    "        horizons: List of forecast horizons to evaluate.\n",
    "        model_keys: List of model keys to process. If None, process all keys in results.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with seasonal results for the specified models.\n",
    "    \"\"\"\n",
    "    seasonal_results = {}\n",
    "\n",
    "    if model_keys is None:\n",
    "        model_keys = list(results.keys())\n",
    "\n",
    "    # Process each model\n",
    "    for key in model_keys:\n",
    "        if key not in results:\n",
    "            print(f\"Warning: Model key '{key}' not found in results. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Filter for growing season\n",
    "        seasonal_model_results = filter_growing_season(results[key])\n",
    "\n",
    "        # Recalculate metrics for the filtered data\n",
    "        seasonal_model_results[\"metrics_by_gauge\"] = recalculate_metrics_from_predictions_df(\n",
    "            seasonal_model_results[\"predictions_df\"], horizons\n",
    "        )\n",
    "\n",
    "        # Store in results dictionary\n",
    "        seasonal_results[key] = seasonal_model_results\n",
    "\n",
    "    return seasonal_results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "seasonal_results = process_seasonal_results(\n",
    "    results, horizons=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], model_keys=list(models_and_datamodules.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(context=\"paper\", font_scale=1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_horizon_performance_bars(\n",
    "    seasonal_results,\n",
    "    horizon=1,\n",
    "    metric=\"NSE\",\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    variants=[\"benchmark\", \"pretrained\", \"finetuned\"],\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    figsize=(12, 5),\n",
    "    with_whiskers=False,\n",
    "    positive_is_better=True,\n",
    "    dummy_model=\"dummy\"\n",
    ")\n",
    "ax.set_ylim(0, 1)\n",
    "sns.despine()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_basin_performance_scatter(\n",
    "    seasonal_results,\n",
    "    benchmark_pattern=\"pretrained\",\n",
    "    challenger_pattern=\"finetuned\",\n",
    "    horizon=10,\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    metric=\"NSE\",\n",
    "    figsize=(10, 6),\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    debug=False,\n",
    ")\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_model_cdf_grid(\n",
    "    seasonal_results,\n",
    "    horizons=[1, 5, 10],\n",
    "    metric=\"NSE\",\n",
    "    architectures=[\"tide\", \"ealstm\", \"tsmixer\", \"tft\"],\n",
    "    variants=[\"benchmark\", \"finetuned\"],\n",
    "    colors={\"tide\": \"#4682B4\", \"ealstm\": \"#CD5C5C\", \"tsmixer\": \"#009E73\", \"tft\": \"#9370DB\"},\n",
    "    figsize=(10, 8),\n",
    ")\n",
    "sns.despine()\n",
    "plt.savefig(f\"/Users/cooper/Desktop/hydro-forecasting/images/preliminary_results/cdf_grid_{COUNTRY}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
