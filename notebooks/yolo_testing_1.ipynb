{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caabe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "from hydro_forecasting.data.in_memory_datamodule import HydroInMemoryDataModule\n",
    "from hydro_forecasting.model_evaluation.evaluators import TSForecastEvaluator\n",
    "from hydro_forecasting.model_evaluation.hp_from_yaml import hp_from_yaml\n",
    "from hydro_forecasting.models.tide import LitTiDE, TiDEConfig\n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545d504",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5d83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = Path(\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/tajikistan/tide.yaml\")\n",
    "\n",
    "tide_hp = hp_from_yaml(\"tide\", yaml_path)\n",
    "tide_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ca = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_ca = CaravanifyParquet(config_ca)\n",
    "basin_ids = caravan_ca.get_all_gauge_ids()[:50]\n",
    "\n",
    "# basin_ids = [bid for bid in basin_ids if bid != \"CA_15030\"]\n",
    "\n",
    "caravan_ca.load_stations(basin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_us = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"USA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_us = CaravanifyParquet(config_us)\n",
    "basin_ids += caravan_us.get_all_gauge_ids()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8755a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34197fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv/USA\",\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes/USA\",\n",
    "}\n",
    "\n",
    "DATA_CHUNK_SIZE = 10\n",
    "\n",
    "datamodule = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/yolo_6\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=tide_hp[\"input_len\"],\n",
    "    output_length=tide_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=basin_ids,\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=DATA_CHUNK_SIZE,\n",
    "    validation_chunk_size=2 * DATA_CHUNK_SIZE,\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29347d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = datamodule.get_train_dataloader()\n",
    "\n",
    "dataset = dataloader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46d6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.chunk_column_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc06754",
   "metadata": {},
   "source": [
    "## Verify static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a807898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = datamodule.test_dataset\n",
    "# if not test_dataset:\n",
    "#     print(\"Test dataset not found.\")\n",
    "# elif len(test_dataset) == 0:\n",
    "#     print(\"Test dataset is empty.\")\n",
    "# else:\n",
    "#     print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "#     # --- Get a Sample ---\n",
    "#     sample_index = 1654\n",
    "#     print(f\"Getting sample at index {sample_index}...\")\n",
    "#     try:\n",
    "#         sample = test_dataset[sample_index]\n",
    "\n",
    "#         # --- Check for NaNs in the Sample Tensors ---\n",
    "#         print(\"\\n--- Checking for NaNs in sample tensors ---\")\n",
    "#         for key, tensor in sample.items():\n",
    "#             if isinstance(tensor, torch.Tensor):\n",
    "#                 has_nan = torch.isnan(tensor).any().item()\n",
    "#                 print(f\"Tensor'{key}' shape: {tensor.shape}, Contains NaNs: {has_nan}\")\n",
    "#                 print(f\"  Sample tensor '{key}': {tensor[:5]}\")\n",
    "#                 if has_nan:\n",
    "#                     # Optional: Print where NaNs occur\n",
    "#                     nan_indices = torch.nonzero(torch.isnan(tensor))\n",
    "#                     print(f\"  NaN indices in '{key}': {nan_indices.tolist()[:5]}...\") # Print first 5\n",
    "#             else:\n",
    "#                 print(f\"Item '{key}' is not a tensor (type: {type(tensor)})\")\n",
    "\n",
    "#     except IndexError:\n",
    "#         print(f\"Error: Index {sample_index} out of bounds for dataset size {len(test_dataset)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while getting or checking the sample: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bea655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie = datamodule.val_index_entries[1661]\n",
    "\n",
    "# file_path = ie[\"file_path\"]\n",
    "# start_idx = ie[\"start_idx\"]\n",
    "# end_idx = ie[\"end_idx\"]\n",
    "# gauge_id = ie[\"gauge_id\"]\n",
    "\n",
    "# data = pd.read_parquet(file_path)\n",
    "\n",
    "# # Slice the data\n",
    "# data_slice = data.iloc[start_idx:end_idx]\n",
    "# print(f\"Data slice shape: {data_slice.shape}\")\n",
    "\n",
    "# data_slice[\"streamflow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5525f5",
   "metadata": {},
   "source": [
    "## Let's try training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = datamodule.input_length\n",
    "output_length = datamodule.output_length\n",
    "\n",
    "config = TiDEConfig(**tide_hp)\n",
    "\n",
    "\n",
    "# Instantiate the Lightning module.\n",
    "model = LitTiDE(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b384c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=5, accelerator=\"gpu\", devices=1, reload_dataloaders_every_n_epochs=True)\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_datamodules = {\n",
    "    \"TiDE\": (model, datamodule),\n",
    "}\n",
    "\n",
    "evaluator = TSForecastEvaluator(\n",
    "    horizons=list(range(1, output_length + 1)),\n",
    "    models_and_datamodules=models_and_datamodules,\n",
    "    trainer_kwargs={\n",
    "        \"accelerator\": \"cpu\",\n",
    "        \"devices\": 1,\n",
    "        # \"reload_dataloaders_every_epoch\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluator.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f23c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"TiDE\"][\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ids = df[\"basin_id\"].unique().to_list()\n",
    "unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the basin ID to filter\n",
    "basin_id = \"CA_15040\"\n",
    "\n",
    "# Filter the dataframe using polars syntax\n",
    "df_basin = df.filter(pl.col(\"basin_id\") == basin_id)\n",
    "\n",
    "# Convert to pandas for plotting (optional, but sometimes easier with matplotlib)\n",
    "# Alternatively, you can use the polars values directly as shown below\n",
    "# df_basin_pd = df_basin.to_pandas()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Use  to access the column values for plotting\n",
    "plt.plot(df_basin[\"date\"], df_basin[\"prediction\"], label=\"Prediction\", color=\"blue\")\n",
    "plt.plot(df_basin[\"date\"], df_basin[\"observed\"], label=\"Observed\", color=\"orange\")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(f\"Observed vs Prediction for {basin_id}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Streamflow\")\n",
    "plt.legend()\n",
    "\n",
    "# Format the x-axis for dates\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "plt.gcf().autofmt_xdate()\n",
    "\n",
    "# Remove the top and right spines\n",
    "sns.despine()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b0dbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
