{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9308b178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent  \n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2caabe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydro_forecasting.data.datamodule import HydroInMemoryDataModule \n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer\n",
    "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "\n",
    "from hydro_forecasting.models.tide import LitTiDE, TiDEConfig\n",
    "from hydro_forecasting.model_evaluation.hp_from_yaml import hp_from_yaml\n",
    "\n",
    "from hydro_forecasting.model_evaluation.evaluators import TSForecastEvaluator\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545d504",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0df5d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder_output_size': 24,\n",
       " 'dropout': 0.4040330172235821,\n",
       " 'future_forcing_projection_size': 0,\n",
       " 'future_input_size': 9,\n",
       " 'group_identifier': 'gauge_id',\n",
       " 'hidden_size': 110,\n",
       " 'input_len': 34,\n",
       " 'input_size': 10,\n",
       " 'learning_rate': 0.00029399848560567596,\n",
       " 'num_decoder_layers': 2,\n",
       " 'num_encoder_layers': 2,\n",
       " 'output_len': 10,\n",
       " 'past_feature_projection_size': 0,\n",
       " 'scheduler_factor': 0.5,\n",
       " 'scheduler_patience': 5,\n",
       " 'static_size': 10,\n",
       " 'temporal_decoder_hidden_size': 51,\n",
       " 'use_layer_norm': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_path = Path(\"/Users/cooper/Desktop/hydro-forecasting/notebooks/tide.yaml\")\n",
    "\n",
    "tide_hp = hp_from_yaml(\"tide\", yaml_path)\n",
    "tide_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0bd441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ca = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_ca = CaravanifyParquet(config_ca)\n",
    "basin_ids = caravan_ca.get_all_gauge_ids()[:10]\n",
    "\n",
    "# basin_ids = [bid for bid in basin_ids if bid != \"CA_15030\"]\n",
    "\n",
    "caravan_ca.load_stations(basin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f1055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_us = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"USA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_us = CaravanifyParquet(config_us)\n",
    "basin_ids += caravan_us.get_all_gauge_ids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8755a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b02753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline(\n",
    "        [(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]\n",
    "    ),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline(\n",
    "        [(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]\n",
    "    ),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34197fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.datamodule:Starting data preparation...\n",
      "INFO:hydro_forecasting.data.datamodule:Generated Run UUID for current config: a0aaa4c5-598c-52b9-a814-edd6bec93b02\n",
      "INFO:hydro_forecasting.data.datamodule:Checking for existing processed data at: /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02\n",
      "INFO:hydro_forecasting.data.datamodule:No reusable data found or reuse failed for UUID a0aaa4c5-598c-52b9-a814-edd6bec93b02. Reason: Run directory not found.. Running preprocessing...\n",
      "INFO:hydro_forecasting.data.preprocessing:Config saved to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02/config.json\n",
      "INFO:hydro_forecasting.data.preprocessing:Processing static features...\n",
      "INFO:hydro_forecasting.data.preprocessing:Static features saved to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02/processed_static_features.parquet\n",
      "INFO:hydro_forecasting.data.preprocessing:Static features pipeline saved to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02/fitted_static_pipeline.joblib\n",
      "INFO:hydro_forecasting.data.preprocessing:Starting time series processing for 20 basins in batches of 50...\n",
      "INFO:hydro_forecasting.data.preprocessing:--- Processing Batch 1 (20 basins) ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Processed 20 basins, 19 passed quality checks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.preprocessing:--- Finished Batch 1 ---\n",
      "INFO:hydro_forecasting.data.preprocessing:Finished processing all time series batches. Attempted 20 basins.\n",
      "INFO:hydro_forecasting.data.preprocessing:Fitted time series pipelines saved to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02/fitted_time_series_pipelines.joblib\n",
      "INFO:hydro_forecasting.data.preprocessing:Summary quality report saved to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02/quality_summary.json\n",
      "INFO:hydro_forecasting.data.preprocessing:SUCCESS: Preprocessing completed successfully. Output at /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/a0aaa4c5-598c-52b9-a814-edd6bec93b02\n",
      "INFO:hydro_forecasting.data.datamodule:Hydro processor completed successfully.\n",
      "INFO:hydro_forecasting.data.datamodule:Loading fitted pipelines...\n",
      "INFO:hydro_forecasting.data.datamodule:Successfully loaded 3 categories of fitted pipelines.\n",
      "INFO:hydro_forecasting.data.datamodule:19 basins retained after processing.\n",
      "INFO:hydro_forecasting.data.datamodule:Found 19 basins for train split.\n",
      "INFO:hydro_forecasting.data.datamodule:Found 19 basins for val split.\n",
      "INFO:hydro_forecasting.data.datamodule:Found 19 basins for test split.\n",
      "INFO:hydro_forecasting.data.datamodule:Data preparation finished.\n",
      "INFO:hydro_forecasting.data.datamodule:Loading static data cache...\n",
      "INFO:hydro_forecasting.data.datamodule:Loaded static data for 20 basins.\n",
      "INFO:hydro_forecasting.data.datamodule:Recomputing chunks from 19 training basins.\n",
      "INFO:hydro_forecasting.data.datamodule:Created 2 chunks.\n",
      "INFO:hydro_forecasting.data.datamodule:Initial training chunks computed: 2 chunks of approx size 10\n"
     ]
    }
   ],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv/USA\",\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes/USA\",\n",
    "}\n",
    "\n",
    "DATA_CHUNK_SIZE = 10 \n",
    "RECOMPUTE_CHUNKS_EVERY = 10 \n",
    "\n",
    "datamodule = HydroInMemoryDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/yolo_6\", # Base dir for processed data\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=tide_hp[\"input_len\"],\n",
    "    output_length=tide_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=target,\n",
    "    preprocessing_configs=preprocessing_config, # Pass the pipeline config dict\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=basin_ids, # List of ALL basins you intend to use\n",
    "    is_autoregressive=True,\n",
    "    chunk_size=DATA_CHUNK_SIZE,             # New argument\n",
    "    recompute_every=RECOMPUTE_CHUNKS_EVERY, # New argument\n",
    "    # load_engine='polars' # Optional: keep default or specify 'pyarrow'\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29347d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cc06754",
   "metadata": {},
   "source": [
    "## Verify static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a807898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = datamodule.test_dataset\n",
    "# if not test_dataset:\n",
    "#     print(\"Test dataset not found.\")\n",
    "# elif len(test_dataset) == 0:\n",
    "#     print(\"Test dataset is empty.\")\n",
    "# else:\n",
    "#     print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "#     # --- Get a Sample ---\n",
    "#     sample_index = 1654\n",
    "#     print(f\"Getting sample at index {sample_index}...\")\n",
    "#     try:\n",
    "#         sample = test_dataset[sample_index]\n",
    "\n",
    "#         # --- Check for NaNs in the Sample Tensors ---\n",
    "#         print(\"\\n--- Checking for NaNs in sample tensors ---\")\n",
    "#         for key, tensor in sample.items():\n",
    "#             if isinstance(tensor, torch.Tensor):\n",
    "#                 has_nan = torch.isnan(tensor).any().item()\n",
    "#                 print(f\"Tensor'{key}' shape: {tensor.shape}, Contains NaNs: {has_nan}\")\n",
    "#                 print(f\"  Sample tensor '{key}': {tensor[:5]}\")\n",
    "#                 if has_nan:\n",
    "#                     # Optional: Print where NaNs occur\n",
    "#                     nan_indices = torch.nonzero(torch.isnan(tensor))\n",
    "#                     print(f\"  NaN indices in '{key}': {nan_indices.tolist()[:5]}...\") # Print first 5\n",
    "#             else:\n",
    "#                 print(f\"Item '{key}' is not a tensor (type: {type(tensor)})\")\n",
    "\n",
    "#     except IndexError:\n",
    "#         print(f\"Error: Index {sample_index} out of bounds for dataset size {len(test_dataset)}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred while getting or checking the sample: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bea655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ie = datamodule.val_index_entries[1661]\n",
    "\n",
    "# file_path = ie[\"file_path\"]\n",
    "# start_idx = ie[\"start_idx\"]\n",
    "# end_idx = ie[\"end_idx\"]\n",
    "# gauge_id = ie[\"gauge_id\"]\n",
    "\n",
    "# data = pd.read_parquet(file_path)\n",
    "\n",
    "# # Slice the data\n",
    "# data_slice = data.iloc[start_idx:end_idx]\n",
    "# print(f\"Data slice shape: {data_slice.shape}\")\n",
    "\n",
    "# data_slice[\"streamflow\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5525f5",
   "metadata": {},
   "source": [
    "## Let's try training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74eb05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = datamodule.input_length\n",
    "output_length = datamodule.output_length\n",
    "\n",
    "config = TiDEConfig(**tide_hp)\n",
    "\n",
    "\n",
    "# Instantiate the Lightning module.\n",
    "model = LitTiDE(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65b384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "INFO:hydro_forecasting.data.datamodule:Data preparation has already run.\n",
      "\n",
      "  | Name          | Type      | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | mse_criterion | MSELoss   | 0      | train\n",
      "1 | model         | TiDEModel | 250 K  | train\n",
      "----------------------------------------------------\n",
      "250 K     Trainable params\n",
      "0         Non-trainable params\n",
      "250 K     Total params\n",
      "1.001     Total estimated model params size (MB)\n",
      "40        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.datamodule:Loading validation data for 19 basins...\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Loading chunk for stage 'val' with 19 basins...\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Chunk data loaded. Shape: (45866, 12). Memory usage: 2.10 MB\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Precomputing index for in-memory chunk...\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15013: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15016: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15020: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15022: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15025: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15034: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15039: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15040: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15044: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01013500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01022500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01030500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01031500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01047000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01052500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01054200: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01055000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01057000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01073000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Chunk loading and indexing complete for stage 'val'. Processed 19, Skipped 0. Found 0 valid samples. Time: 0.21s\n",
      "WARNING:hydro_forecasting.data.datamodule:Validation dataset is empty after loading. Returning empty dataloader.\n",
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:106: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "INFO:hydro_forecasting.data.datamodule:Epoch 0: Loading training chunk 1/2 with 10 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Loading chunk for stage 'train' with 10 basins...\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Chunk data loaded. Shape: (44200, 12). Memory usage: 2.02 MB\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Precomputing index for in-memory chunk...\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01057000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01052500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01055000: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15034: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15020: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15039: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15040: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for USA_01030500: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15025: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "WARNING:hydro_forecasting.data.in_memory_dataset:Could not find valid sequences for CA_15016: NumPy calculation error: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n",
      "INFO:hydro_forecasting.data.in_memory_dataset:Chunk loading and indexing complete for stage 'train'. Processed 10, Skipped 0. Found 0 valid samples. Time: 0.12s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m trainer = pl.Trainer(\n\u001b[32m      4\u001b[39m     max_epochs=\u001b[32m5\u001b[39m,\n\u001b[32m      5\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     devices=\u001b[32m1\u001b[39m,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m trainer.strategy.launcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     51\u001b[39m     _call_teardown_hook(trainer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    592\u001b[39m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28mself\u001b[39m._signal_connector.register_signal_handlers()\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1017\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1054\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_sanity_check()\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected state \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:208\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip:\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:237\u001b[39m, in \u001b[36m_FitLoop.setup_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    234\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: resetting train dataloader\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    236\u001b[39m source = \u001b[38;5;28mself\u001b[39m._data_source\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m train_dataloader = \u001b[43m_request_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m trainer.strategy.barrier(\u001b[33m\"\u001b[39m\u001b[33mtrain_dataloader()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataloader, CombinedLoader):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:326\u001b[39m, in \u001b[36m_request_dataloader\u001b[39m\u001b[34m(data_source)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[32m    316\u001b[39m \n\u001b[32m    317\u001b[39m \u001b[33;03mReturns:\u001b[39;00m\n\u001b[32m    318\u001b[39m \u001b[33;03m    The requested dataloader\u001b[39;00m\n\u001b[32m    319\u001b[39m \n\u001b[32m    320\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[32m    322\u001b[39m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[32m    323\u001b[39m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[32m    324\u001b[39m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[32m    325\u001b[39m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_source\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:293\u001b[39m, in \u001b[36m_DataLoaderSource.dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.instance, pl.LightningDataModule):\n\u001b[32m    292\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.instance.trainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.instance\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:198\u001b[39m, in \u001b[36m_call_lightning_datamodule_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[32m    197\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.datamodule.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/data/datamodule.py:365\u001b[39m, in \u001b[36mHydroInMemoryDataModule.train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    346\u001b[39m train_dataset = InMemoryChunkDataset(\n\u001b[32m    347\u001b[39m     basin_ids=current_basin_ids,\n\u001b[32m    348\u001b[39m     processed_data_dir=\u001b[38;5;28mself\u001b[39m.processed_time_series_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m    358\u001b[39m     load_engine=\u001b[38;5;28mself\u001b[39m.load_engine,\n\u001b[32m    359\u001b[39m )\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# Use a standard RandomSampler for shuffling within the chunk\u001b[39;00m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\n\u001b[32m    363\u001b[39m     train_dataset,\n\u001b[32m    364\u001b[39m     batch_size=\u001b[38;5;28mself\u001b[39m.batch_size,\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     sampler=\u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Shuffle samples within the chunk\u001b[39;00m\n\u001b[32m    366\u001b[39m     num_workers=\u001b[38;5;28mself\u001b[39m.num_workers,\n\u001b[32m    367\u001b[39m     pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    368\u001b[39m     persistent_workers=\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_workers > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    369\u001b[39m     \u001b[38;5;66;03m# worker_init_fn=worker_init_fn, # May need custom init for chunking\u001b[39;00m\n\u001b[32m    370\u001b[39m     \u001b[38;5;66;03m# multiprocessing_context=mp.get_context(\"spawn\"),\u001b[39;00m\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/torch/utils/data/sampler.py:156\u001b[39m, in \u001b[36mRandomSampler.__init__\u001b[39m\u001b[34m(self, data_source, replacement, num_samples, generator)\u001b[39m\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.replacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m     )\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_samples <= \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    157\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    158\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_datamodules = {\n",
    "    \"TiDE\": (model, datamodule),\n",
    "}\n",
    "\n",
    "evaluator = TSForecastEvaluator(\n",
    "    horizons=list(range(1, output_length + 1)),\n",
    "    models_and_datamodules=models_and_datamodules,\n",
    "    trainer_kwargs={\n",
    "        \"accelerator\": \"cpu\",\n",
    "        \"devices\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f5de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluator.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f23c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results[\"TiDE\"][\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dcd14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f78d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "basin_id = \"CA_15013\"\n",
    "df_basin = df[df[\"basin_id\"] == basin_id]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_basin[\"date\"], df_basin[\"prediction\"], label=\"Prediction\", color=\"blue\")\n",
    "plt.plot(df_basin[\"date\"], df_basin[\"observed\"], label=\"Observed\", color=\"orange\")\n",
    "plt.title(f\"Observed vs Prediction for {basin_id}\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Streamflow\")\n",
    "plt.legend()\n",
    "plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "plt.gcf().autofmt_xdate()\n",
    "sns.despine()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75872aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e45c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
