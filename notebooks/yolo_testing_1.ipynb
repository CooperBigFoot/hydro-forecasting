{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9308b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /Users/cooper/Desktop/hydro-forecasting/src to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch \n",
    "import pandas as pd\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent  \n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caabe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydro_forecasting.data.lazy_datamodule import HydroLazyDataModule\n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "\n",
    "from hydro_forecasting.models.ealstm import LitEALSTM, EALSTMConfig\n",
    "from hydro_forecasting.model_evaluation.evaluators import TSForecastEvaluator\n",
    "from hydro_forecasting.model_evaluation.hp_from_yaml import hp_from_yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a545d504",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df5d83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bias': True,\n",
       " 'bidirectional': True,\n",
       " 'bidirectional_fusion': 'concat',\n",
       " 'dropout': 0.09091248360355031,\n",
       " 'future_hidden_size': 79,\n",
       " 'future_input_size': 9,\n",
       " 'future_layers': 3,\n",
       " 'group_identifier': 'gauge_id',\n",
       " 'hidden_size': 79,\n",
       " 'input_len': 36,\n",
       " 'input_size': 10,\n",
       " 'learning_rate': 0.0008706020878304854,\n",
       " 'num_layers': 3,\n",
       " 'output_len': 10,\n",
       " 'scheduler_factor': 0.5,\n",
       " 'scheduler_patience': 5,\n",
       " 'static_size': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaml_path = Path(\"/Users/cooper/Desktop/hydro-forecasting/notebooks/ealstm.yaml\")\n",
    "\n",
    "ealstm_hp = hp_from_yaml(\"ealstm\", yaml_path)\n",
    "ealstm_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0bd441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_ca = CaravanifyParquetConfig(\n",
    "    attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "    timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "    shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/CA/post_processed/shapefiles\",\n",
    "    gauge_id_prefix=\"CA\",\n",
    "    use_hydroatlas_attributes=True,\n",
    "    use_caravan_attributes=True,\n",
    "    use_other_attributes=True,\n",
    ")\n",
    "\n",
    "caravan_ca = CaravanifyParquet(config_ca)\n",
    "basin_ids = caravan_ca.get_all_gauge_ids()[:10]\n",
    "\n",
    "# basin_ids = [bid for bid in basin_ids if bid != \"CA_15030\"]\n",
    "\n",
    "caravan_ca.load_stations(basin_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f1055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_us = CaravanifyParquetConfig(\n",
    "#     attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes\",\n",
    "#     timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv\",\n",
    "#     shapefile_dir=\"/Users/cooper/Desktop/CAMELS-CH/data/CARAVANIFY/USA/post_processed/shapefiles\",\n",
    "#     gauge_id_prefix=\"USA\",\n",
    "#     use_hydroatlas_attributes=True,\n",
    "#     use_caravan_attributes=True,\n",
    "#     use_other_attributes=True,\n",
    "# )\n",
    "\n",
    "# caravan_us = CaravanifyParquet(config_us)\n",
    "# basin_ids += caravan_us.get_all_gauge_ids()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8755a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    # \"gauge_id\",\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b02753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34197fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ STARTING PREPROCESSING PIPELINE ================\n",
      "\n",
      "================ LOADING STATIC ATTRIBUTES ================\n",
      "INFO: Attempting to load static attributes for 10 gauge IDs\n",
      "INFO: Processing static attributes for region 'CA'\n",
      "INFO: Loaded caravan attributes for 10 gauges in CA\n",
      "INFO: Loaded hydroatlas attributes for 10 gauges in CA\n",
      "INFO: Loaded other attributes for 10 gauges in CA\n",
      "INFO: Horizontally merging 3 attribute files for region 'CA'\n",
      "INFO: Vertically stacking attribute data from 1 regions\n",
      "SUCCESS: Loaded and merged static attributes for 10 unique basins from 3 files across 1 regions.\n",
      "INFO: Successfully loaded static attributes for 10 basins.\n",
      "\n",
      "================ FITTING PREPROCESSING PIPELINES ================\n",
      "INFO: Loading time series data for pipeline fitting...\n",
      "INFO: Loaded time series data for 10 basins\n",
      "INFO: Split time series data into train (33798), val (16896), test (16910)\n",
      "INFO: Fitted 3 pipelines\n",
      "\n",
      "================ PROCESSING BASIN TIME SERIES ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 10/10 [00:00<00:00, 12.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ PROCESSING STATIC ATTRIBUTES ================\n",
      "INFO: Processing static attributes...\n",
      "SUCCESS: Saved transformed static attributes for 9 basins to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_static_data.parquet\n",
      "\n",
      "================ PROCESSING SUMMARY ================\n",
      "SUCCESS: Completed processing 9 of 10 basins\n",
      "WARNING: 1 basins excluded due to quality issues\n",
      "INFO: Created training dataset with 32429 samples\n",
      "INFO: Created validation dataset with 16435 samples\n",
      "INFO: Created test dataset with 16249 samples\n"
     ]
    }
   ],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/timeseries/csv/USA\",\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    \"CA\": \"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes/CA\",\n",
    "    \"USA\": \"/Users/cooper/Desktop/CaravanifyParquet/USA/post_processed/attributes/USA\",\n",
    "}\n",
    "\n",
    "datamodule = HydroLazyDataModule(\n",
    "    region_time_series_base_dirs=region_time_series_base_dirs,\n",
    "    region_static_attributes_base_dirs=region_static_attributes_base_dirs,\n",
    "    path_to_preprocessing_output_directory=\"/Users/cooper/Desktop/hydro-forecasting/tests/yolo_6\",\n",
    "    group_identifier=\"gauge_id\",\n",
    "    batch_size=2048,\n",
    "    input_length=ealstm_hp[\"input_len\"],\n",
    "    output_length=ealstm_hp[\"output_len\"],\n",
    "    forcing_features=forcing_features,\n",
    "    static_features=static_features,\n",
    "    target=\"streamflow\",\n",
    "    preprocessing_configs=preprocessing_config,\n",
    "    num_workers=4,\n",
    "    min_train_years=5,\n",
    "    train_prop=0.5,\n",
    "    val_prop=0.25,\n",
    "    test_prop=0.25,\n",
    "    max_imputation_gap_size=5,\n",
    "    list_of_gauge_ids_to_process=basin_ids,\n",
    "    is_autoregressive=True,\n",
    "    files_per_batch=20,\n",
    ")\n",
    "\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc06754",
   "metadata": {},
   "source": [
    "## Verify static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a807898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 16249\n",
      "Getting sample at index 10...\n",
      "\n",
      "--- Checking for NaNs in sample tensors ---\n",
      "<class 'torch.Tensor'> 'X' shape: torch.Size([36, 10]), Contains NaNs: False\n",
      "<class 'torch.Tensor'> 'y' shape: torch.Size([10]), Contains NaNs: False\n",
      "<class 'torch.Tensor'> 'static' shape: torch.Size([10]), Contains NaNs: False\n",
      "<class 'torch.Tensor'> 'future' shape: torch.Size([10, 9]), Contains NaNs: False\n",
      "Item 'gauge_id' is not a tensor (type: <class 'str'>)\n",
      "Item 'input_end_date' is not a tensor (type: <class 'int'>)\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datamodule.test_dataset\n",
    "if not test_dataset:\n",
    "    print(\"Test dataset not found.\")\n",
    "elif len(test_dataset) == 0:\n",
    "    print(\"Test dataset is empty.\")\n",
    "else:\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "    # --- Get a Sample ---\n",
    "    sample_index = 10\n",
    "    print(f\"Getting sample at index {sample_index}...\")\n",
    "    try:\n",
    "        sample = test_dataset[sample_index]\n",
    "\n",
    "        # --- Check for NaNs in the Sample Tensors ---\n",
    "        print(\"\\n--- Checking for NaNs in sample tensors ---\")\n",
    "        for key, tensor in sample.items():\n",
    "            if isinstance(tensor, torch.Tensor):\n",
    "                has_nan = torch.isnan(tensor).any().item()\n",
    "                print(f\"{type(tensor)} '{key}' shape: {tensor.shape}, Contains NaNs: {has_nan}\")\n",
    "                if has_nan:\n",
    "                    # Optional: Print where NaNs occur\n",
    "                    nan_indices = torch.nonzero(torch.isnan(tensor))\n",
    "                    print(f\"  NaN indices in '{key}': {nan_indices.tolist()[:5]}...\") # Print first 5\n",
    "            else:\n",
    "                print(f\"Item '{key}' is not a tensor (type: {type(tensor)})\")\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"Error: Index {sample_index} out of bounds for dataset size {len(test_dataset)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while getting or checking the sample: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10864892",
   "metadata": {},
   "source": [
    "## Verify time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "043a0081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01057000.parquet\n",
      "NaN values found in 'streamflow' column of /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15034.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01022500.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15025.parquet\n",
      "NaN values found in 'streamflow' column of /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15013.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01055000.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01073000.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01054200.parquet\n",
      "NaN values found in 'streamflow' column of /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15044.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01031500.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15022.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01047000.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15040.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15039.parquet\n",
      "NaN values found in 'streamflow' column of /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15016.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/CA_15020.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01030500.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01052500.parquet\n",
      "'streamflow' column is valid in /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data/USA_01013500.parquet\n"
     ]
    }
   ],
   "source": [
    "path_to_ts = Path(\"/Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_data\")\n",
    "\n",
    "# Verify that the \"streamflow\" column exists in the .parquet files and does not contain NaNs\n",
    "def check_streamflow_in_parquet_files(path_to_ts):\n",
    "    for file in path_to_ts.glob(\"*.parquet\"):\n",
    "        df = pd.read_parquet(file)\n",
    "        if \"streamflow\" not in df.columns:\n",
    "            print(f\"'streamflow' column not found in {file}\")\n",
    "        else:\n",
    "            if df[\"streamflow\"].isnull().any():\n",
    "                print(f\"NaN values found in 'streamflow' column of {file}\")\n",
    "            else:\n",
    "                print(f\"'streamflow' column is valid in {file}\")\n",
    "check_streamflow_in_parquet_files(path_to_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5525f5",
   "metadata": {},
   "source": [
    "## Let's try training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74eb05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = datamodule.input_length\n",
    "output_length = datamodule.output_length\n",
    "\n",
    "config = EALSTMConfig(**ealstm_hp)\n",
    "\n",
    "\n",
    "# Instantiate the Lightning module.\n",
    "model = LitEALSTM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b384c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ STARTING PREPROCESSING PIPELINE ================\n",
      "\n",
      "================ LOADING STATIC ATTRIBUTES ================\n",
      "INFO: Attempting to load static attributes for 9 gauge IDs\n",
      "INFO: Processing static attributes for region 'CA'\n",
      "INFO: Loaded caravan attributes for 9 gauges in CA\n",
      "INFO: Loaded hydroatlas attributes for 9 gauges in CA\n",
      "INFO: Loaded other attributes for 9 gauges in CA\n",
      "INFO: Horizontally merging 3 attribute files for region 'CA'\n",
      "INFO: Vertically stacking attribute data from 1 regions\n",
      "SUCCESS: Loaded and merged static attributes for 9 unique basins from 3 files across 1 regions.\n",
      "INFO: Successfully loaded static attributes for 9 basins.\n",
      "\n",
      "================ FITTING PREPROCESSING PIPELINES ================\n",
      "INFO: Loading time series data for pipeline fitting...\n",
      "INFO: Loaded time series data for 9 basins\n",
      "INFO: Split time series data into train (33153), val (16574), test (16586)\n",
      "INFO: Fitted 3 pipelines\n",
      "\n",
      "================ PROCESSING BASIN TIME SERIES ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing basins: 100%|██████████| 9/9 [00:00<00:00, 11.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ PROCESSING STATIC ATTRIBUTES ================\n",
      "INFO: Processing static attributes...\n",
      "SUCCESS: Saved transformed static attributes for 9 basins to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_static_data.parquet\n",
      "\n",
      "================ PROCESSING SUMMARY ================\n",
      "SUCCESS: Completed processing 9 of 9 basins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name          | Type     | Params | Mode \n",
      "---------------------------------------------------\n",
      "0 | mse_criterion | MSELoss  | 0      | train\n",
      "1 | model         | BiEALSTM | 191 K  | train\n",
      "---------------------------------------------------\n",
      "191 K     Trainable params\n",
      "0         Non-trainable params\n",
      "191 K     Total params\n",
      "0.767     Total estimated model params size (MB)\n",
      "58        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Created training dataset with 32429 samples\n",
      "INFO: Created validation dataset with 16435 samples\n",
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (16) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 16/16 [00:07<00:00,  2.15it/s, v_num=43]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 16/16 [00:07<00:00,  2.14it/s, v_num=43]\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e04f3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_and_datamodules = {\n",
    "    \"EALSTM\": (model, datamodule),\n",
    "}\n",
    "\n",
    "evaluator = TSForecastEvaluator(\n",
    "    horizons=list(range(1, output_length + 1)),\n",
    "    models_and_datamodules=models_and_datamodules,\n",
    "    trainer_kwargs={\n",
    "        \"accelerator\": \"cpu\",\n",
    "        \"devices\": 1,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10f5de30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing EALSTM...\n",
      "\n",
      "================ STARTING PREPROCESSING PIPELINE ================\n",
      "\n",
      "================ LOADING STATIC ATTRIBUTES ================\n",
      "INFO: Attempting to load static attributes for 9 gauge IDs\n",
      "INFO: Processing static attributes for region 'CA'\n",
      "INFO: Loaded caravan attributes for 9 gauges in CA\n",
      "INFO: Loaded hydroatlas attributes for 9 gauges in CA\n",
      "INFO: Loaded other attributes for 9 gauges in CA\n",
      "INFO: Horizontally merging 3 attribute files for region 'CA'\n",
      "INFO: Vertically stacking attribute data from 1 regions\n",
      "SUCCESS: Loaded and merged static attributes for 9 unique basins from 3 files across 1 regions.\n",
      "INFO: Successfully loaded static attributes for 9 basins.\n",
      "\n",
      "================ FITTING PREPROCESSING PIPELINES ================\n",
      "INFO: Loading time series data for pipeline fitting...\n",
      "INFO: Loaded time series data for 9 basins\n",
      "INFO: Split time series data into train (33153), val (16574), test (16586)\n",
      "INFO: Fitted 3 pipelines\n",
      "\n",
      "================ PROCESSING BASIN TIME SERIES ================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cooper/Desktop/hydro-forecasting/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Processing basins: 100%|██████████| 9/9 [00:00<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ PROCESSING STATIC ATTRIBUTES ================\n",
      "INFO: Processing static attributes...\n",
      "SUCCESS: Saved transformed static attributes for 9 basins to /Users/cooper/Desktop/hydro-forecasting/tests/yolo_6/processed_static_data.parquet\n",
      "\n",
      "================ PROCESSING SUMMARY ================\n",
      "SUCCESS: Completed processing 9 of 9 basins\n",
      "INFO: Created test dataset with 16249 samples\n",
      "Testing DataLoader 0: 100%|██████████| 8/8 [00:02<00:00,  3.52it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_loss                   nan\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "DEBUG: Raw predictions contain NaNs: True\n",
      "DEBUG: Raw observations contain NaNs: True\n",
      "Evaluating results with shape: preds=(16249, 10), obs=(16249, 10), basin_ids=(16249,)\n"
     ]
    }
   ],
   "source": [
    "results = evaluator.test_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86f23c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon</th>\n",
       "      <th>prediction</th>\n",
       "      <th>observed</th>\n",
       "      <th>basin_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>CA_15013</td>\n",
       "      <td>2017-05-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>CA_15013</td>\n",
       "      <td>2017-05-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>CA_15013</td>\n",
       "      <td>2017-05-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.559999</td>\n",
       "      <td>CA_15013</td>\n",
       "      <td>2017-05-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.559999</td>\n",
       "      <td>CA_15013</td>\n",
       "      <td>2017-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162485</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>CA_15044</td>\n",
       "      <td>2022-12-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162486</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>CA_15044</td>\n",
       "      <td>2022-12-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162487</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>CA_15044</td>\n",
       "      <td>2022-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162488</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>CA_15044</td>\n",
       "      <td>2022-12-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162489</th>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>CA_15044</td>\n",
       "      <td>2022-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162490 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        horizon  prediction  observed  basin_id       date\n",
       "0             1         NaN  8.000000  CA_15013 2017-05-28\n",
       "1             2         NaN  7.500000  CA_15013 2017-05-29\n",
       "2             3         NaN  7.500000  CA_15013 2017-05-30\n",
       "3             4         NaN  6.559999  CA_15013 2017-05-31\n",
       "4             5         NaN  6.559999  CA_15013 2017-06-01\n",
       "...         ...         ...       ...       ...        ...\n",
       "162485        6         NaN  0.860000  CA_15044 2022-12-27\n",
       "162486        7         NaN  0.860000  CA_15044 2022-12-28\n",
       "162487        8         NaN  0.860000  CA_15044 2022-12-29\n",
       "162488        9         NaN  0.860000  CA_15044 2022-12-30\n",
       "162489       10         NaN  0.860000  CA_15044 2022-12-31\n",
       "\n",
       "[162490 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[\"EALSTM\"][\"df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115eef9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8f78d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
