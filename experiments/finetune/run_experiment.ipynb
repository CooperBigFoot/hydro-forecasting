{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "2f2fc10e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# !jupyter lab build"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "659bed6b",
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Added /Users/cooper/Desktop/hydro-forecasting/src to Python path\n"
               ]
            }
         ],
         "source": [
            "import sys\n",
            "from pathlib import Path\n",
            "\n",
            "# Add src directory to Python path\n",
            "project_root = Path.cwd().parent.parent\n",
            "src_path = project_root / \"src\"\n",
            "if str(src_path) not in sys.path:\n",
            "    sys.path.insert(0, str(src_path))\n",
            "    print(f\"Added {src_path} to Python path\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "f65efe2c",
         "metadata": {},
         "outputs": [],
         "source": [
            "import torch\n",
            "from sklearn.pipeline import Pipeline\n",
            "\n",
            "from hydro_forecasting.data.caravanify_parquet import (\n",
            "    CaravanifyParquet,\n",
            "    CaravanifyParquetConfig,\n",
            ")\n",
            "from hydro_forecasting.experiment_utils.finetune_pretrained_model import finetune_pretrained_models\n",
            "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
            "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
            "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5e7c3393",
         "metadata": {},
         "source": [
            "---"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "dab68b34",
         "metadata": {},
         "source": [
            "## Experiment constants"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "7a6885fe",
         "metadata": {},
         "outputs": [],
         "source": [
            "REGIONS = [\n",
            "    \"CA\"\n",
            "]\n",
            "\n",
            "COUNTRY = \"tajikistan\""
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2533b81f",
         "metadata": {},
         "source": [
            "## Loading the data (as gauge_ids)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "id": "ed0ce646",
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "['CA_16205',\n",
                     " 'CA_17050',\n",
                     " 'CA_17077',\n",
                     " 'CA_17082',\n",
                     " 'CA_17100',\n",
                     " 'CA_17110',\n",
                     " 'CA_17137',\n",
                     " 'CA_17147',\n",
                     " 'CA_17150',\n",
                     " 'CA_17202',\n",
                     " 'CA_17288',\n",
                     " 'CA_17325',\n",
                     " 'CA_17329',\n",
                     " 'CA_17338',\n",
                     " 'CA_17344',\n",
                     " 'CA_17453']"
                  ]
               },
               "execution_count": 14,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "def load_basin_ids(country: str) -> list[str]:\n",
            "    \"\"\"\n",
            "    Function to load basins for a given country in Central Asia\n",
            "    \"\"\"\n",
            "    # Make country lowercase and make the first letter uppercase\n",
            "    country = country.lower()\n",
            "    country = country.capitalize()\n",
            "\n",
            "    if country != \"Tajikistan\" and country != \"Kyrgyzstan\":\n",
            "        print(\"Country not supported\")\n",
            "        return []\n",
            "\n",
            "    configs = CaravanifyParquetConfig(\n",
            "        attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
            "        timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
            "        gauge_id_prefix=\"CA\",\n",
            "        use_hydroatlas_attributes=True,\n",
            "        use_caravan_attributes=True,\n",
            "        use_other_attributes=True,\n",
            "    )\n",
            "\n",
            "    caravan = CaravanifyParquet(configs)\n",
            "    ca_basins = caravan.get_all_gauge_ids()\n",
            "    caravan.load_stations(ca_basins)\n",
            "    static_data = caravan.get_static_attributes()\n",
            "\n",
            "    return list(static_data[static_data[\"country\"] == country][\"gauge_id\"].unique())\n",
            "\n",
            "basin_ids = load_basin_ids(COUNTRY)\n",
            "basin_ids"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "73f9edf0",
         "metadata": {},
         "source": [
            "## Datamodule Configs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "c2ea76fb",
         "metadata": {},
         "outputs": [],
         "source": [
            "region_time_series_base_dirs = {\n",
            "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/timeseries/csv/{region}\"\n",
            "    for region in REGIONS\n",
            "}\n",
            "\n",
            "region_static_attributes_base_dirs = {\n",
            "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/attributes/{region}\" for region in REGIONS\n",
            "}\n",
            "\n",
            "path_to_preprocessing_output_directory = (\n",
            "    \"/Users/cooper/Desktop/hydro-forecasting/experiments/finetune/data_cache/{COUNTRY}\"\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "f14911d3",
         "metadata": {},
         "outputs": [],
         "source": [
            "forcing_features = [\n",
            "    \"snow_depth_water_equivalent_mean\",\n",
            "    \"surface_net_solar_radiation_mean\",\n",
            "    \"surface_net_thermal_radiation_mean\",\n",
            "    \"potential_evaporation_sum_ERA5_LAND\",\n",
            "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
            "    \"temperature_2m_mean\",\n",
            "    \"temperature_2m_min\",\n",
            "    \"temperature_2m_max\",\n",
            "    \"total_precipitation_sum\",\n",
            "]\n",
            "\n",
            "static_features = [\n",
            "    \"p_mean\",\n",
            "    \"area\",\n",
            "    \"ele_mt_sav\",\n",
            "    \"high_prec_dur\",\n",
            "    \"frac_snow\",\n",
            "    \"high_prec_freq\",\n",
            "    \"slp_dg_sav\",\n",
            "    \"cly_pc_sav\",\n",
            "    \"aridity_ERA5_LAND\",\n",
            "    \"aridity_FAO_PM\",\n",
            "]\n",
            "\n",
            "target = \"streamflow\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "962da35a",
         "metadata": {},
         "outputs": [],
         "source": [
            "feature_pipeline = GroupedPipeline(\n",
            "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
            "    columns=forcing_features,\n",
            "    group_identifier=\"gauge_id\",\n",
            ")\n",
            "\n",
            "target_pipeline = GroupedPipeline(\n",
            "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
            "    columns=[\"streamflow\"],\n",
            "    group_identifier=\"gauge_id\",\n",
            ")\n",
            "\n",
            "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
            "\n",
            "preprocessing_config = {\n",
            "    \"features\": {\"pipeline\": feature_pipeline},\n",
            "    \"target\": {\"pipeline\": target_pipeline},\n",
            "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
            "}"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "id": "cd0f0b66",
         "metadata": {},
         "outputs": [],
         "source": [
            "datamodule_config = {\n",
            "    \"region_time_series_base_dirs\": region_time_series_base_dirs,\n",
            "    \"region_static_attributes_base_dirs\": region_static_attributes_base_dirs,\n",
            "    \"path_to_preprocessing_output_directory\": path_to_preprocessing_output_directory,\n",
            "    \"group_identifier\": \"gauge_id\",\n",
            "    \"batch_size\": 2048,\n",
            "    \"forcing_features\": forcing_features,\n",
            "    \"static_features\": static_features,\n",
            "    \"target\": target,\n",
            "    \"num_workers\": 4,\n",
            "    \"min_train_years\": 10,\n",
            "    \"train_prop\": 0.5,\n",
            "    \"val_prop\": 0.25,\n",
            "    \"test_prop\": 0.25,\n",
            "    \"max_imputation_gap_size\": 5,\n",
            "    \"chunk_size\": 100,\n",
            "    \"validation_chunk_size\": 100,\n",
            "    \"is_autoregressive\": True,\n",
            "    \"preprocessing_configs\": preprocessing_config,\n",
            "}"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "158363b3",
         "metadata": {},
         "source": [
            "## Training Configs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "id": "9709710c",
         "metadata": {},
         "outputs": [],
         "source": [
            "training_config = {\n",
            "    \"max_epochs\": 200,\n",
            "    \"accelerator\": \"mps\",\n",
            "    \"devices\": 1,\n",
            "    \"early_stopping_patience\": 15,\n",
            "    \"reload_dataloaders_every_n_epochs\": False,\n",
            "}"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "38258592",
         "metadata": {},
         "source": [
            "## Remaining Configs"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "id": "2274e894",
         "metadata": {},
         "outputs": [],
         "source": [
            "output_dir = \"/Users/cooper/Desktop/hydro-forecasting/experiments/finetune\"\n",
            "pretrained_checkpoint_dir = (\n",
            "    \"/Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints\"\n",
            ")\n",
            "\n",
            "model_types = [\"tsmixer\", \"tide\", \"ealstm\"]\n",
            "yaml_paths = [\n",
            "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tide.yaml\",\n",
            "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY}/ealstm.yaml\",\n",
            "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tsmixer.yaml\",\n",
            "    # f\"/Users/cooper/Desktop/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tft.yaml\",\n",
            "]\n",
            "experiment_name = f\"finetune_{COUNTRY}\"\n",
            "num_runs = 1\n",
            "override_previous_attempts = False"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "fb369289",
         "metadata": {},
         "source": [
            "## Training the models from scratch"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "id": "1054c874",
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "INFO:hydro_forecasting.experiment_utils.finetune_pretrained_model:Found pre-trained checkpoint for tsmixer: /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tsmixer/run_0/attempt_0/tsmixer-run0-attempt_0-epoch=135-val_loss=0.0388.ckpt\n",
                  "INFO:hydro_forecasting.experiment_utils.finetune_pretrained_model:Found pre-trained checkpoint for tide: /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tide/run_0/attempt_0/tide-run0-attempt_0-epoch=65-val_loss=0.0504.ckpt\n",
                  "INFO:hydro_forecasting.experiment_utils.finetune_pretrained_model:Found pre-trained checkpoint for ealstm: /Users/cooper/Desktop/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/ealstm/run_0/attempt_0/ealstm-run0-attempt_0-epoch=141-val_loss=0.0349.ckpt\n"
               ]
            },
            {
               "ename": "ValueError",
               "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_results = \u001b[43mfinetune_pretrained_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgauge_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasin_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_checkpoint_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatamodule_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatamodule_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_yaml_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43myaml_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverride_previous_attempts\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverride_previous_attempts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr_reduction_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mselect_best_from_pretrained\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/experiment_utils/finetune_pretrained_model.py:177\u001b[39m, in \u001b[36mfinetune_pretrained_models\u001b[39m\u001b[34m(gauge_ids, pretrained_checkpoint_dir, model_types, pretrained_yaml_paths, datamodule_config, training_config, output_dir, experiment_name, select_best_from_pretrained, pretrained_run_index, pretrained_attempt_index, lr_reduction_factor, num_runs, base_seed, override_previous_attempts)\u001b[39m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Failure(\u001b[33m\"\u001b[39m\u001b[33mNo models with valid pre-trained checkpoints found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Run the experiment for valid models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m result = \u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_model_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43myaml_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_yaml_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_provider_fns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalid_provider_fns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgauge_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgauge_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# Merge results\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, Success):\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hydro-forecasting/src/hydro_forecasting/experiment_utils/training_runner.py:416\u001b[39m, in \u001b[36mExperimentRunner.run_experiment\u001b[39m\u001b[34m(self, model_types, yaml_paths, model_provider_fns, gauge_ids)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_experiment\u001b[39m(\n\u001b[32m    397\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    398\u001b[39m     model_types: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    401\u001b[39m     gauge_ids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m    402\u001b[39m ) -> Result[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]], \u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    403\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[33;03m    Run training experiment for multiple model types.\u001b[39;00m\n\u001b[32m    405\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    414\u001b[39m \u001b[33;03m        (best_checkpoint_path, metrics_dict) if successful, or an error message if failed.\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gauge_ids:\n\u001b[32m    417\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Failure(\u001b[33m\"\u001b[39m\u001b[33mNo gauge IDs provided for training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    419\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_types) != \u001b[38;5;28mlen\u001b[39m(yaml_paths) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(model_types) != \u001b[38;5;28mlen\u001b[39m(model_provider_fns):\n",
                  "\u001b[31mValueError\u001b[39m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
               ]
            }
         ],
         "source": [
            "train_results = finetune_pretrained_models(\n",
            "    gauge_ids=basin_ids,\n",
            "    pretrained_checkpoint_dir=pretrained_checkpoint_dir,\n",
            "    datamodule_config=datamodule_config,\n",
            "    training_config=training_config,\n",
            "    output_dir=output_dir,\n",
            "    model_types=model_types,\n",
            "    pretrained_yaml_paths=yaml_paths,\n",
            "    experiment_name=experiment_name,\n",
            "    num_runs=num_runs,\n",
            "    override_previous_attempts=override_previous_attempts,\n",
            "    lr_reduction_factor=10,\n",
            "    select_best_from_pretrained=True\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "e96de1cc",
         "metadata": {},
         "outputs": [],
         "source": []
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.8"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}
