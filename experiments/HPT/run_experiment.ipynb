{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2fc10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter lab build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent.parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65efe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "from hydro_forecasting.experiment_utils.hyperparameter_tune_model import hyperparameter_tune_model\n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c3393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab68b34",
   "metadata": {},
   "source": [
    "## Experiment constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6885fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\n",
    "    \"CA\"\n",
    "]\n",
    "\n",
    "COUNTRY = \"tajikistan\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533b81f",
   "metadata": {},
   "source": [
    "## Loading the data (as gauge_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_basin_ids(country: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Function to load basins for a given country in Central Asia\n",
    "    \"\"\"\n",
    "    # Make country lowercase and make the first letter uppercase\n",
    "    country = country.lower()\n",
    "    country = country.capitalize()\n",
    "\n",
    "    if country != \"Tajikistan\" and country != \"Kyrgyzstan\":\n",
    "        print(\"Country not supported\")\n",
    "        return []\n",
    "\n",
    "    configs = CaravanifyParquetConfig(\n",
    "        attributes_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/attributes\",\n",
    "        timeseries_dir=\"/Users/cooper/Desktop/CaravanifyParquet/CA/post_processed/timeseries/csv\",\n",
    "        gauge_id_prefix=\"CA\",\n",
    "        use_hydroatlas_attributes=True,\n",
    "        use_caravan_attributes=True,\n",
    "        use_other_attributes=True,\n",
    "    )\n",
    "\n",
    "    caravan = CaravanifyParquet(configs)\n",
    "    ca_basins = caravan.get_all_gauge_ids()\n",
    "    caravan.load_stations(ca_basins)\n",
    "    static_data = caravan.get_static_attributes()\n",
    "\n",
    "    return list(static_data[static_data[\"country\"] == country][\"gauge_id\"].unique())\n",
    "\n",
    "basin_ids = load_basin_ids(COUNTRY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9edf0",
   "metadata": {},
   "source": [
    "## Datamodule Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ea76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/timeseries/csv/{region}\"\n",
    "    for region in REGIONS\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    region: f\"/Users/cooper/Desktop/CaravanifyParquet/{region}/post_processed/attributes/{region}\" for region in REGIONS\n",
    "}\n",
    "\n",
    "path_to_preprocessing_output_directory = (\n",
    "    f\"/Users/cooper/Desktop/hydro-forecasting/experiments/HPT/data_cache/{COUNTRY}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14911d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_config = {\n",
    "    \"output_length\": 10,\n",
    "    \"region_time_series_base_dirs\": region_time_series_base_dirs,\n",
    "    \"region_static_attributes_base_dirs\": region_static_attributes_base_dirs,\n",
    "    \"path_to_preprocessing_output_directory\": path_to_preprocessing_output_directory,\n",
    "    \"group_identifier\": \"gauge_id\",\n",
    "    \"batch_size\": 2048,\n",
    "    \"forcing_features\": forcing_features,\n",
    "    \"static_features\": static_features,\n",
    "    \"target\": target,\n",
    "    \"num_workers\": 4,\n",
    "    \"min_train_years\": 5,\n",
    "    \"train_prop\": 0.5,\n",
    "    \"val_prop\": 0.25,\n",
    "    \"test_prop\": 0.25,\n",
    "    \"max_imputation_gap_size\": 5,\n",
    "    \"chunk_size\": 100,\n",
    "    \"validation_chunk_size\": 100,\n",
    "    \"is_autoregressive\": True,\n",
    "    \"preprocessing_configs\": preprocessing_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158363b3",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9709710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"max_epochs\": 200,\n",
    "    \"accelerator\": \"mps\",\n",
    "    \"devices\": 1,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"reload_dataloaders_every_n_epochs\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38258592",
   "metadata": {},
   "source": [
    "## Remaining Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/Users/cooper/Desktop/hydro-forecasting/experiments/HPT\"\n",
    "model_type = \"tide\"\n",
    "experiment_name = f\"HPT_{model_type}_{COUNTRY}\"\n",
    "num_trials = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb369289",
   "metadata": {},
   "source": [
    "## Training the models from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt_study = hyperparameter_tune_model(\n",
    "    model_type=model_type,\n",
    "    gauge_ids=basin_ids,\n",
    "    datamodule_config=datamodule_config,\n",
    "    training_config=training_config,\n",
    "    output_dir_study=output_dir,\n",
    "    experiment_name=experiment_name,\n",
    "    n_trials=num_trials,\n",
    "    search_spaces_dir=\"/Users/cooper/Desktop/hydro-forecasting/experiments/HPT/search-spaces\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96de1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Set your parameters here - corrected path and study name\n",
    "db_path = \"/Users/cooper/Desktop/hydro-forecasting/experiments/HPT/HPT_tide_tajikistan/HPT_tide_tajikistan_study.db\"\n",
    "study_name = \"HPT_tide_tajikistan\"  # Use exact name from logs\n",
    "output_yaml = \"/Users/cooper/Desktop/hydro-forecasting/experiments/HPT/best_params.yaml\"\n",
    "\n",
    "\n",
    "# Define analysis function\n",
    "def analyze_optuna_study(storage_path, study_name):\n",
    "    \"\"\"Analyze an Optuna study and return details about the best trial.\"\"\"\n",
    "    # Format the storage URL if needed\n",
    "    storage_url = f\"sqlite:///{storage_path}\" if not storage_path.startswith(\"sqlite:///\") else storage_path\n",
    "\n",
    "    # Load the study\n",
    "    print(f\"Loading study '{study_name}' from {storage_url}\")\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage_url)\n",
    "\n",
    "    # Show summary\n",
    "    print(f\"\\nStudy Summary:\")\n",
    "    print(f\"  Number of trials: {len(study.trials)}\")\n",
    "    print(f\"  Best value: {study.best_value}\")\n",
    "    print(f\"  Direction: {study.direction}\")\n",
    "\n",
    "    if not study.trials:\n",
    "        print(\"No trials found in study.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Get the best trial\n",
    "    best_trial = study.best_trial\n",
    "    print(f\"\\nBest Trial (#{best_trial.number}):\")\n",
    "    print(f\"  Value: {best_trial.value}\")\n",
    "\n",
    "    # Display best hyperparameters\n",
    "    print(\"\\nBest Hyperparameters:\")\n",
    "    for param_name, param_value in best_trial.params.items():\n",
    "        print(f\"  {param_name}: {param_value}\")\n",
    "\n",
    "    # Get model configuration\n",
    "    model_config = best_trial.params.copy()\n",
    "\n",
    "    # Add user attributes if they exist\n",
    "    if hasattr(best_trial, \"user_attrs\"):\n",
    "        if \"model_config\" in best_trial.user_attrs:\n",
    "            model_config = best_trial.user_attrs[\"model_config\"]\n",
    "        elif \"hparams\" in best_trial.user_attrs:\n",
    "            model_config = best_trial.user_attrs[\"hparams\"]\n",
    "\n",
    "    return study, best_trial, model_config\n",
    "\n",
    "\n",
    "# Run the analysis\n",
    "study, best_trial, best_config = analyze_optuna_study(db_path, study_name)\n",
    "\n",
    "# Save to YAML if needed\n",
    "if best_config:\n",
    "    yaml_path = Path(output_yaml)\n",
    "    print(f\"\\nSaving best model config to {yaml_path}\")\n",
    "    with open(yaml_path, \"w\") as f:\n",
    "        yaml.dump(best_config, f)\n",
    "\n",
    "# Plot the optimization history if there are enough trials\n",
    "if study and len(study.trials) > 1:\n",
    "    # Get dataframe of all trials\n",
    "    df = study.trials_dataframe()\n",
    "\n",
    "    # Plot optimization history\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_title(\"Optimization History\")\n",
    "    ax.set_xlabel(\"Trial Number\")\n",
    "    ax.set_ylabel(\"Objective Value (val_loss)\")\n",
    "    ax.plot(df[\"number\"], df[\"value\"], \"o-\")\n",
    "    plt.show()\n",
    "\n",
    "    # Try to plot parameter importances if more than 5 trials\n",
    "    if len(study.trials) >= 5:\n",
    "        try:\n",
    "            importances = optuna.importance.get_param_importances(study)\n",
    "            importance_df = pd.DataFrame(importances.items(), columns=[\"Parameter\", \"Importance\"])\n",
    "            importance_df = importance_df.sort_values(\"Importance\", ascending=False)\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "            ax.barh(importance_df[\"Parameter\"], importance_df[\"Importance\"])\n",
    "            ax.set_title(\"Parameter Importances\")\n",
    "            ax.set_xlabel(\"Importance\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate parameter importances: {e}\")\n",
    "else:\n",
    "    print(\"\\nNot enough trials to plot optimization history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f95a4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
