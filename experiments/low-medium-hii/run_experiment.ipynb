{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f2fc10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabBuildApp] JupyterLab 4.0.8\n",
      "[LabBuildApp] Building in /usr/local/share/jupyter/lab\n",
      "[LabBuildApp] ERROR | Build failed.\n",
      "Troubleshooting: If the build failed due to an out-of-memory error, you\n",
      "may be able to fix it by disabling the `dev_build` and/or `minimize` options.\n",
      "\n",
      "If you are building via the `jupyter lab build` command, you can disable\n",
      "these options like so:\n",
      "\n",
      "jupyter lab build --dev-build=False --minimize=False\n",
      "\n",
      "You can also disable these options for all JupyterLab builds by adding these\n",
      "lines to a Jupyter config file named `jupyter_config.py`:\n",
      "\n",
      "c.LabBuildApp.minimize = False\n",
      "c.LabBuildApp.dev_build = False\n",
      "\n",
      "If you don't already have a `jupyter_config.py` file, you can create one by\n",
      "adding a blank file of that name to any of the Jupyter config directories.\n",
      "The config directories can be listed by running:\n",
      "\n",
      "jupyter --paths\n",
      "\n",
      "Explanation:\n",
      "\n",
      "- `dev-build`: This option controls whether a `dev` or a more streamlined\n",
      "`production` build is used. This option will default to `False` (i.e., the\n",
      "`production` build) for most users. However, if you have any labextensions\n",
      "installed from local files, this option will instead default to `True`.\n",
      "Explicitly setting `dev-build` to `False` will ensure that the `production`\n",
      "build is used in all circumstances.\n",
      "\n",
      "- `minimize`: This option controls whether your JS bundle is minified\n",
      "during the Webpack build, which helps to improve JupyterLab's overall\n",
      "performance. However, the minifier plugin used by Webpack is very memory\n",
      "intensive, so turning it off may help the build finish successfully in\n",
      "low-memory environments.\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/jupyterlab/debuglog.py:56: UserWarning: An error occurred.\n",
      "  warnings.warn(\"An error occurred.\")\n",
      "/usr/local/lib/python3.10/dist-packages/jupyterlab/debuglog.py:57: UserWarning: ValueError: Please install Node.js and npm before continuing installation. You may be able to install Node.js from your package manager, from conda, or directly from the Node.js website (https://nodejs.org).\n",
      "  warnings.warn(msg[-1].strip())\n",
      "/usr/local/lib/python3.10/dist-packages/jupyterlab/debuglog.py:58: UserWarning: See the log file for details: /tmp/jupyterlab-debug-j4dd8k_c.log\n",
      "  warnings.warn(f\"See the log file for details: {log_path!s}\")\n"
     ]
    }
   ],
   "source": [
    "!jupyter lab build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659bed6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd().parent.parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "    print(f\"Added {src_path} to Python path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f65efe2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hydro_forecasting.data.caravanify_parquet import (\n",
    "    CaravanifyParquet,\n",
    "    CaravanifyParquetConfig,\n",
    ")\n",
    "from hydro_forecasting.preprocessing.grouped import GroupedPipeline\n",
    "from hydro_forecasting.preprocessing.normalize import NormalizeTransformer\n",
    "from hydro_forecasting.preprocessing.standard_scale import StandardScaleTransformer\n",
    "from hydro_forecasting.experiment_utils.train_model_from_scratch import train_model_from_scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c3393",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab68b34",
   "metadata": {},
   "source": [
    "## Experiment constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6885fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGIONS = [\n",
    "    \"CL\",\n",
    "    \"CH\",\n",
    "    \"USA\",\n",
    "    \"camelsaus\",\n",
    "    \"camelsgb\",\n",
    "    \"camelsbr\",\n",
    "    \"hysets\",\n",
    "    \"lamah\",\n",
    "]\n",
    "\n",
    "COUNTRY = \"tajikistan\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2533b81f",
   "metadata": {},
   "source": [
    "## Loading the data (as gauge_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0ce646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading basin IDs for region: CL\n",
      "Original gauge_ids: 505\n",
      "Filtered gauge_ids: 487\n",
      "Loading basin IDs for region: CH\n",
      "Original gauge_ids: 135\n",
      "Filtered gauge_ids: 0\n",
      "No gauge_ids matched the specified human influence categories.\n",
      "Loading basin IDs for region: USA\n",
      "Original gauge_ids: 671\n",
      "Filtered gauge_ids: 567\n",
      "Loading basin IDs for region: camelsaus\n",
      "Original gauge_ids: 222\n",
      "Filtered gauge_ids: 215\n",
      "Loading basin IDs for region: camelsgb\n",
      "Original gauge_ids: 671\n",
      "Filtered gauge_ids: 538\n",
      "Loading basin IDs for region: camelsbr\n",
      "Original gauge_ids: 870\n",
      "Filtered gauge_ids: 868\n",
      "Loading basin IDs for region: hysets\n",
      "Original gauge_ids: 12162\n",
      "Filtered gauge_ids: 8463\n",
      "Loading basin IDs for region: lamah\n",
      "Original gauge_ids: 859\n",
      "Filtered gauge_ids: 812\n"
     ]
    }
   ],
   "source": [
    "def load_basin_ids(regions, human_influence_path: str, human_influence_categories: list):\n",
    "    \"\"\"\n",
    "    Simple function to load basin IDs from multiple regions and filter by human influence.\n",
    "\n",
    "    Args:\n",
    "        regions: List of region names to process\n",
    "        human_influence_path: Path to human influence classification file\n",
    "        human_influence_categories: Categories of human influence to keep\n",
    "\n",
    "    Returns:\n",
    "        List of filtered basin IDs\n",
    "    \"\"\"\n",
    "\n",
    "    human_influence_path = Path(human_influence_path)\n",
    "\n",
    "    basin_ids = []\n",
    "\n",
    "    for region in regions:\n",
    "        print(f\"Loading basin IDs for region: {region}\")\n",
    "        # Set up paths\n",
    "        attributes_dir = f\"/workspace/CaravanifyParquet/{region}/post_processed/attributes\"\n",
    "        timeseries_dir = f\"/workspace/CaravanifyParquet/{region}/post_processed/timeseries/csv\"\n",
    "\n",
    "        # Create config\n",
    "        config = CaravanifyParquetConfig(\n",
    "            attributes_dir=attributes_dir,\n",
    "            timeseries_dir=timeseries_dir,\n",
    "            human_influence_path=human_influence_path,\n",
    "            gauge_id_prefix=region,\n",
    "            use_hydroatlas_attributes=True,\n",
    "            use_caravan_attributes=True,\n",
    "            use_other_attributes=True,\n",
    "        )\n",
    "\n",
    "        # Get and filter basin IDs\n",
    "        caravan = CaravanifyParquet(config)\n",
    "        region_basin_ids = caravan.get_all_gauge_ids()\n",
    "\n",
    "        # Filter by human influence if path provided\n",
    "        if human_influence_path and human_influence_path.exists():\n",
    "            filtered_ids, _ = caravan.filter_gauge_ids_by_human_influence(region_basin_ids, human_influence_categories)\n",
    "        else:\n",
    "            filtered_ids = region_basin_ids\n",
    "\n",
    "        basin_ids.extend(filtered_ids)\n",
    "\n",
    "    # Return unique, sorted basin IDs\n",
    "    return sorted(set(basin_ids))\n",
    "\n",
    "\n",
    "basin_ids = load_basin_ids(\n",
    "    REGIONS,\n",
    "    human_influence_path=\"/workspace/hydro-forecasting/scripts/human_influence_index/results/human_influence_classification.parquet\",\n",
    "    human_influence_categories=[\"Low\", \"Medium\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9edf0",
   "metadata": {},
   "source": [
    "## Datamodule Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ea76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_time_series_base_dirs = {\n",
    "    region: f\"/workspace/CaravanifyParquet/{region}/post_processed/timeseries/csv/{region}\"\n",
    "    for region in REGIONS\n",
    "}\n",
    "\n",
    "region_static_attributes_base_dirs = {\n",
    "    region: f\"/workspace/CaravanifyParquet/{region}/post_processed/attributes/{region}\" for region in REGIONS\n",
    "}\n",
    "\n",
    "path_to_preprocessing_output_directory = \"/workspace/hydro-forecasting/experiments/low-medium-hii/data_cache\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f14911d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_features = [\n",
    "    \"snow_depth_water_equivalent_mean\",\n",
    "    \"surface_net_solar_radiation_mean\",\n",
    "    \"surface_net_thermal_radiation_mean\",\n",
    "    \"potential_evaporation_sum_ERA5_LAND\",\n",
    "    \"potential_evaporation_sum_FAO_PENMAN_MONTEITH\",\n",
    "    \"temperature_2m_mean\",\n",
    "    \"temperature_2m_min\",\n",
    "    \"temperature_2m_max\",\n",
    "    \"total_precipitation_sum\",\n",
    "]\n",
    "\n",
    "static_features = [\n",
    "    \"p_mean\",\n",
    "    \"area\",\n",
    "    \"ele_mt_sav\",\n",
    "    \"high_prec_dur\",\n",
    "    \"frac_snow\",\n",
    "    \"high_prec_freq\",\n",
    "    \"slp_dg_sav\",\n",
    "    \"cly_pc_sav\",\n",
    "    \"aridity_ERA5_LAND\",\n",
    "    \"aridity_FAO_PM\",\n",
    "]\n",
    "\n",
    "target = \"streamflow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962da35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=forcing_features,\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "target_pipeline = GroupedPipeline(\n",
    "    Pipeline([(\"scaler\", StandardScaleTransformer()), (\"normalizer\", NormalizeTransformer())]),\n",
    "    columns=[\"streamflow\"],\n",
    "    group_identifier=\"gauge_id\",\n",
    ")\n",
    "\n",
    "static_pipeline = Pipeline([(\"scaler\", StandardScaleTransformer())])\n",
    "\n",
    "preprocessing_config = {\n",
    "    \"features\": {\"pipeline\": feature_pipeline},\n",
    "    \"target\": {\"pipeline\": target_pipeline},\n",
    "    \"static_features\": {\"pipeline\": static_pipeline, \"columns\": static_features},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0f0b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule_config = {\n",
    "    \"region_time_series_base_dirs\": region_time_series_base_dirs,\n",
    "    \"region_static_attributes_base_dirs\": region_static_attributes_base_dirs,\n",
    "    \"path_to_preprocessing_output_directory\": path_to_preprocessing_output_directory,\n",
    "    \"group_identifier\": \"gauge_id\",\n",
    "    \"batch_size\": 2048,\n",
    "    \"forcing_features\": forcing_features,\n",
    "    \"static_features\": static_features,\n",
    "    \"target\": target,\n",
    "    \"num_workers\": 10,\n",
    "    \"min_train_years\": 10,\n",
    "    \"train_prop\": 0.5,\n",
    "    \"val_prop\": 0.25,\n",
    "    \"test_prop\": 0.25,\n",
    "    \"max_imputation_gap_size\": 5,\n",
    "    \"chunk_size\": 1500,\n",
    "    \"validation_chunk_size\": 4000,\n",
    "    \"is_autoregressive\": True,\n",
    "    \"preprocessing_configs\": preprocessing_config,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158363b3",
   "metadata": {},
   "source": [
    "## Training Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9709710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"max_epochs\": 300,\n",
    "    \"accelerator\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"devices\": 1,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"reload_dataloaders_every_n_epochs\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38258592",
   "metadata": {},
   "source": [
    "## Remaining Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2274e894",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/workspace/hydro-forecasting/experiments/low-medium-hii\"\n",
    "model_types = [\"tide\", \"ealstm\", \"tsmixer\", \"tft\"]\n",
    "yaml_paths = [\n",
    "    f\"/workspace/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tide.yaml\",\n",
    "    f\"/workspace/hydro-forecasting/experiments/yaml-files/{COUNTRY}/ealstm.yaml\",\n",
    "    f\"/workspace/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tsmixer.yaml\",\n",
    "    f\"/workspace/hydro-forecasting/experiments/yaml-files/{COUNTRY}/tft.yaml\",\n",
    "]\n",
    "experiment_name = f\"low-medium-hii_{COUNTRY}\"\n",
    "num_runs = 1\n",
    "override_previous_attempts = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb369289",
   "metadata": {},
   "source": [
    "## Training the models from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054c874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Starting training for experiment 'low-medium-hii_tajikistan'\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Output directory: /workspace/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Models to train: tide, ealstm, tsmixer, tft\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Number of runs per model: 1\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Processing model (1/4): tide\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Starting data preparation...\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Generated Run UUID for current config: e82931a3-45c7-56ef-a2a8-4f74f03e347d\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Checking for existing processed data at: /workspace/hydro-forecasting/experiments/low-medium-hii/data_cache/e82931a3-45c7-56ef-a2a8-4f74f03e347d\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Successfully validated and prepared to reuse data from /workspace/hydro-forecasting/experiments/low-medium-hii/data_cache/e82931a3-45c7-56ef-a2a8-4f74f03e347d\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Reusing existing processed data from run_uuid: e82931a3-45c7-56ef-a2a8-4f74f03e347d\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Loaded 3 pipelines and data for 8329 basins from reused run.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Found 6473 basins for synchronized train/val chunking and validation pool selection.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Found 6473 basins for test split.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Data preparation finished.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Loading static data cache and converting to Tensors...\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Loaded and tensorized static data for 11950 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Created fixed validation pool with 4000 basins: ['hysets_05LL015', 'hysets_10336674', 'USA_02231342', 'hysets_05RB003', 'camelsbr_61175000']...\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Loading and caching data for 4000 validation basins...\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'val' chunk data loaded for 4000 basins. Shape: (12904112, 12). Est. Mem: 590.76 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Successfully cached validation data. Index entries: 12472112.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Initializing/Re-initializing training shared chunks from 6473 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Created 4 training shared chunks.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Initial training shared chunks: 4 chunks of approx size 2000 from 6473 basins.\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Starting run 1/1 for tide\n",
      "Seed set to 42\n",
      "INFO:hydro_forecasting.experiment_utils.train_model_from_scratch:Using seed: 42\n",
      "INFO:hydro_forecasting.experiment_utils.checkpoint_manager:Creating new attempt: /workspace/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/checkpoints/tide/run_0/attempt_0\n",
      "INFO:hydro_forecasting.experiment_utils.checkpoint_manager:Creating new attempt: /workspace/hydro-forecasting/experiments/low-medium-hii/low-medium-hii_tajikistan/logs/tide/run_0/attempt_0\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Data preparation has already run.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type      | Params | Mode \n",
      "----------------------------------------------------\n",
      "0 | mse_criterion | MSELoss   | 0      | train\n",
      "1 | model         | TiDEModel | 266 K  | train\n",
      "----------------------------------------------------\n",
      "266 K     Trainable params\n",
      "0         Non-trainable params\n",
      "266 K     Total params\n",
      "1.065     Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43934d7f23d44d7a97d06b0c1e2e6f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 0: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n",
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: The combination of `DataLoader(`pin_memory=True`, `persistent_workers=True`) and `Trainer(reload_dataloaders_every_n_epochs > 0)` can lead to instability due to limitations in PyTorch (https://github.com/pytorch/pytorch/issues/91252). We recommend setting `pin_memory=False` in this case.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 0: Train Dataloader using chunk 1/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12900815, 12). Est. Mem: 590.58 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fea969a55f7416a97ac809d15f4fd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cb1c610bde74420a95783f71e9c4c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.161\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 1: Train Dataloader using chunk 2/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12937903, 12). Est. Mem: 592.28 MB\n",
      "WARNING:hydro_forecasting.data.in_memory_datamodule:Could not find valid sequences for basin hysets_07030280 in stage 'train': Data height (101) is less than total sequence length (109)\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 1: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d40050029634dc98a91ddfbca22dafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.083 >= min_delta = 0.0. New best score: 0.078\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 2: Train Dataloader using chunk 3/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12962450, 12). Est. Mem: 593.40 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 2: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4053c35c6c54e8aa725b064ab8543c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.004 >= min_delta = 0.0. New best score: 0.074\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 3: Train Dataloader using chunk 4/4 with 473 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 473 basins. Shape: (3023514, 12). Est. Mem: 138.41 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 3: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dc2685792849dca90ae0b19cb7d9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.in_memory_datamodule:Completed full pass through training shared chunks. Recomputing.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Initializing/Re-initializing training shared chunks from 6473 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Created 4 training shared chunks.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 4: Train Dataloader using chunk 1/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12984509, 12). Est. Mem: 594.41 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 4: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2e79a736394b48bd78197190accf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.064\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 5: Train Dataloader using chunk 2/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12852909, 12). Est. Mem: 588.39 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 5: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a0b9c27c7264684a82e6391837b348b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.064\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 6: Train Dataloader using chunk 3/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12996859, 12). Est. Mem: 594.98 MB\n",
      "WARNING:hydro_forecasting.data.in_memory_datamodule:Could not find valid sequences for basin hysets_07030280 in stage 'train': Data height (101) is less than total sequence length (109)\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 6: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f47f2b1c1644b0ba648522a72a7593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.058\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 7: Train Dataloader using chunk 4/4 with 473 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 473 basins. Shape: (2990405, 12). Est. Mem: 136.90 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 7: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32534b39c77403ba77f480288f68dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.in_memory_datamodule:Completed full pass through training shared chunks. Recomputing.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Initializing/Re-initializing training shared chunks from 6473 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Created 4 training shared chunks.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 8: Train Dataloader using chunk 1/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12933175, 12). Est. Mem: 592.06 MB\n",
      "WARNING:hydro_forecasting.data.in_memory_datamodule:Could not find valid sequences for basin hysets_07030280 in stage 'train': Data height (101) is less than total sequence length (109)\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 8: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64152bdc0b4f62a3df41bc2dbb459c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 9: Train Dataloader using chunk 2/4 with 2000 basins.\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Stage 'train' chunk data loaded for 2000 basins. Shape: (12859267, 12). Est. Mem: 588.68 MB\n",
      "INFO:hydro_forecasting.data.in_memory_datamodule:Epoch 9: Val Dataloader using cached validation data with 12472112 samples from 4000 basins.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d57725e21848faa1f82fdd7b840132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_results = train_model_from_scratch(\n",
    "    gauge_ids=basin_ids,\n",
    "    datamodule_config=datamodule_config,\n",
    "    training_config=training_config,\n",
    "    output_dir=output_dir,\n",
    "    model_types=model_types,\n",
    "    yaml_paths=yaml_paths,\n",
    "    experiment_name=experiment_name,\n",
    "    num_runs=num_runs,\n",
    "    override_previous_attempts=override_previous_attempts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2fbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44febf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
